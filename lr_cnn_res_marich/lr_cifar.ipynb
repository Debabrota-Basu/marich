{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1334,"status":"ok","timestamp":1683836960024,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"FCNYmftHHMRy","outputId":"009c31bd-00ad-4d2c-91df-010c34943cc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]}],"source":["from utils import marich, dataset\n","from nets import *\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import pickle\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7542,"status":"ok","timestamp":1683836967564,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"VWTlQi4DHMRy","outputId":"5b22dd97-afbd-43ed-f56a-a4890e234253"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["logreg = LogisticRegression(784,10).cuda()\n","logreg.load_state_dict(torch.load(\"./targets/logreg_mnist.pt\"))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683836221506,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"mCQwTA1RHMRz"},"outputs":[],"source":["# unlab_x = []\n","# unlab_y = []\n","# for j,k in cifar10:\n","#     j = j.to(device)\n","#     unlab_x.append(j)\n","#     unlab_y.append(torch.argmax(logreg(j.view(j.size(0),-1))))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1683836967565,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"4dJ2tDfLe9bd"},"outputs":[],"source":["# with open('./imagenet_target/cifar_lr_x.pkl', 'wb') as f:\n","#   pickle.dump(unlab_x, f)\n","\n","# with open('./imagenet_target/cifar_lr_y.pkl', 'wb') as f:\n","#   pickle.dump(unlab_y, f)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yt6PfwWae9be"},"outputs":[],"source":["with open('./imagenet_target/cifar_lr_x.pkl', 'rb') as f:\n","  unlab_x = pickle.load(f)\n","\n","with open('./imagenet_target/cifar_lr_y.pkl', 'rb') as f:\n","  unlab_y = pickle.load(f)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class basic_dataset(Dataset):\n","    def __init__(self, X, Y = None, transform = None):\n","        self.X = X\n","        self.Y = Y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        if self.Y != None:\n","            y = self.Y[idx]\n","            return x, y\n","        else:\n","            return x\n","\n","class dataset2(Dataset):\n","    def __init__(self, X, Y = None, transform = None):\n","        self.X = X\n","        self.Y = Y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        if self.Y != None:\n","            y = self.Y[idx]\n","            return x, y\n","        else:\n","            return x\n","    def get_data(self,idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        return x\n","    def get_dataset(self, idx):\n","        x = self.X[idx]\n","        y = torch.tensor(self.Y)[idx]\n","        return basic_dataset(x,y)\n","    def get_label(self,idx):\n","        y = torch.tensor(self.Y)[idx]\n","        return y\n","    def get_data_label_loader(self,idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        y = torch.tensor(self.Y)[idx]\n","        \n","        return_set = basic_dataset(x,y)\n","        return torch.utils.data.DataLoader(return_set, batch_size = 64)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_id, val_id = train_test_split(range(50000), test_size = 0.2)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["unlab_dataset_train = dataset2(torch.stack(unlab_x)[train_id], torch.tensor(unlab_y)[train_id])\n","unlab_dataset_val = dataset2(torch.stack(unlab_x)[val_id], torch.tensor(unlab_y)[val_id])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"G42zMl0wHMR0"},"outputs":[],"source":["testset = datasets.MNIST('../mnist/train/', download=True, train=False, transform=transform)\n","# trainset, testset = torch.utils.data.random_split(trainset, [50000, 10000])\n","# valset = datasets.MNIST('../mnist/test/', download=True, train=False, transform=transform)\n","testloader = DataLoader(testset, batch_size = 256)\n","validloader = DataLoader(unlab_dataset_val, batch_size = 256)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":30291,"status":"error","timestamp":1683754979048,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"klbgtKOBHMR1","outputId":"7fd122fb-dba0-4755-ded3-5588bb007a55"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n"]},{"name":"stdout","output_type":"stream","text":["Using CUDA\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  12.91\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.302399158477783\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  2.281303083896637\n","Epoch time -----  0.8238873481750488  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  2.2297022342681885\n","Validation loss:  2.1816062569618224\n","Epoch time -----  0.9000110626220703  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.9188801050186157\n","Validation loss:  2.1592198491096495\n","Epoch time -----  0.8296511173248291  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8133198618888855\n","Validation loss:  2.137375223636627\n","Epoch time -----  0.8438944816589355  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7493327856063843\n","Validation loss:  2.1248925924301147\n","Epoch time -----  0.8545258045196533  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7116338014602661\n","Validation loss:  2.1154193341732026\n","Epoch time -----  0.7639219760894775  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7189344763755798\n","Validation loss:  2.1091142773628233\n","Epoch time -----  0.6472504138946533  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.67814439535141\n","Validation loss:  2.103732740879059\n","Epoch time -----  0.6351745128631592  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7038461565971375\n","Validation loss:  2.0974338591098785\n","Epoch time -----  0.6263725757598877  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6543057560920715\n","Validation loss:  2.0933793246746064\n","Epoch time -----  0.6186764240264893  sec\n","validation loss minimum, saving model\n","Test accuracy:  27.37\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  102  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  184\n","Training samples:  184\n","Training\n","Epoch:  1\n","Train loss:  1.9067923625310261\n","Validation loss:  2.0847414672374724\n","Epoch time -----  0.8872833251953125  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8572241067886353\n","Validation loss:  2.0625425547361376\n","Epoch time -----  0.668755054473877  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7807964483896892\n","Validation loss:  2.0556203067302703\n","Epoch time -----  0.7373766899108887  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7455135186513264\n","Validation loss:  2.042026859521866\n","Epoch time -----  0.8982305526733398  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7088255087534587\n","Validation loss:  2.02837755382061\n","Epoch time -----  0.8768248558044434  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6831538677215576\n","Validation loss:  2.0198739886283876\n","Epoch time -----  0.9106786251068115  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6615075667699177\n","Validation loss:  2.0069597333669664\n","Epoch time -----  0.9271674156188965  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6406654516855876\n","Validation loss:  1.993221604824066\n","Epoch time -----  0.9011545181274414  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6224213043848674\n","Validation loss:  1.981200784444809\n","Epoch time -----  0.8707275390625  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6045558055241902\n","Validation loss:  1.9698459297418593\n","Epoch time -----  0.8439640998840332  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  50.48\n","Round:  2\n","Using entropy sampling on  39816  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  102  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  267\n","Training samples:  267\n","Training\n","Epoch:  1\n","Train loss:  1.7796785593032838\n","Validation loss:  1.9608410805463792\n","Epoch time -----  0.9144740104675293  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.732386302947998\n","Validation loss:  1.9557785868644715\n","Epoch time -----  0.9110372066497803  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7471567869186402\n","Validation loss:  1.9418523132801055\n","Epoch time -----  0.6339576244354248  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.70229229927063\n","Validation loss:  1.9352059334516525\n","Epoch time -----  0.6488320827484131  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.683621382713318\n","Validation loss:  1.9292047768831253\n","Epoch time -----  0.6471340656280518  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6839425325393678\n","Validation loss:  1.922918716073036\n","Epoch time -----  0.6463229656219482  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6473052978515625\n","Validation loss:  1.915523275732994\n","Epoch time -----  0.6502416133880615  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6243908882141114\n","Validation loss:  1.9097155272960662\n","Epoch time -----  0.6225807666778564  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6425044536590576\n","Validation loss:  1.9045675426721573\n","Epoch time -----  0.662452220916748  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6115055084228516\n","Validation loss:  1.9007602959871293\n","Epoch time -----  0.6596152782440186  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  61.59\n","Round:  3\n","Using entropy sampling on  39733  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  351\n","Training samples:  351\n","Training\n","Epoch:  1\n","Train loss:  1.7554295261700947\n","Validation loss:  1.8939755827188491\n","Epoch time -----  0.8904979228973389  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7224954962730408\n","Validation loss:  1.888127291202545\n","Epoch time -----  0.8600280284881592  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7015235026677449\n","Validation loss:  1.8767629146575928\n","Epoch time -----  0.8782181739807129  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6795552770296733\n","Validation loss:  1.8714234799146652\n","Epoch time -----  0.6333985328674316  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6730884114901226\n","Validation loss:  1.8677916318178176\n","Epoch time -----  0.6721184253692627  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.652945597966512\n","Validation loss:  1.8611278742551804\n","Epoch time -----  0.6252532005310059  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6391102075576782\n","Validation loss:  1.8537242263555527\n","Epoch time -----  0.6611647605895996  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.638855775197347\n","Validation loss:  1.8501243770122529\n","Epoch time -----  0.6160025596618652  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6102529962857564\n","Validation loss:  1.846531230211258\n","Epoch time -----  0.6287448406219482  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6066035429636638\n","Validation loss:  1.8440077155828476\n","Epoch time -----  0.6082146167755127  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  73.13\n","Round:  4\n","Using entropy sampling on  39649  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  436\n","Training samples:  436\n","Training\n","Epoch:  1\n","Train loss:  1.7242268323898315\n","Validation loss:  1.8408622533082961\n","Epoch time -----  0.872161865234375  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7038581201008387\n","Validation loss:  1.8342801123857497\n","Epoch time -----  1.053910732269287  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6834229912076677\n","Validation loss:  1.8307195603847504\n","Epoch time -----  1.019923210144043  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.669417483466012\n","Validation loss:  1.8240276098251342\n","Epoch time -----  0.949127197265625  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6551156214305334\n","Validation loss:  1.8185469537973404\n","Epoch time -----  0.8837423324584961  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6414057867867606\n","Validation loss:  1.8140832871198653\n","Epoch time -----  0.9426755905151367  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.631305353982108\n","Validation loss:  1.8115262597799302\n","Epoch time -----  0.910639762878418  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.621415172304426\n","Validation loss:  1.8074046462774276\n","Epoch time -----  0.8640451431274414  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6120444876807076\n","Validation loss:  1.8048332631587982\n","Epoch time -----  0.8843734264373779  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6011307750429427\n","Validation loss:  1.8036607444286346\n","Epoch time -----  0.9234030246734619  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  69.44\n","Round:  5\n","Using entropy sampling on  39564  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  520\n","Training samples:  520\n","Training\n","Epoch:  1\n","Train loss:  1.6968641016218398\n","Validation loss:  1.7932359904050827\n","Epoch time -----  0.8969485759735107  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.671769791179233\n","Validation loss:  1.7909158557653426\n","Epoch time -----  0.8632080554962158  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6729176176918878\n","Validation loss:  1.7850914120674133\n","Epoch time -----  0.8434109687805176  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6663089725706313\n","Validation loss:  1.7820753186941147\n","Epoch time -----  0.8830263614654541  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.638637277815077\n","Validation loss:  1.7754758059978486\n","Epoch time -----  0.8825497627258301  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6259851455688477\n","Validation loss:  1.772595429420471\n","Epoch time -----  0.86016845703125  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6346642308764987\n","Validation loss:  1.7672725737094879\n","Epoch time -----  0.7710294723510742  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6260606977674696\n","Validation loss:  1.7651529997587203\n","Epoch time -----  0.6679372787475586  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6151117881139119\n","Validation loss:  1.7628696799278258\n","Epoch time -----  0.6668617725372314  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6153188678953383\n","Validation loss:  1.761008095741272\n","Epoch time -----  0.6675739288330078  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.617435826195611\n","Validation loss:  1.7594468891620636\n","Epoch time -----  0.6709113121032715  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.25\n","Round:  6\n","Using entropy sampling on  39480  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  604\n","Training samples:  604\n","Training\n","Epoch:  1\n","Train loss:  1.68629230260849\n","Validation loss:  1.755066579580307\n","Epoch time -----  0.8549952507019043  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6697947978973389\n","Validation loss:  1.7496430218219756\n","Epoch time -----  0.8582522869110107  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6566052079200744\n","Validation loss:  1.747186264395714\n","Epoch time -----  0.8609991073608398  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6381306171417236\n","Validation loss:  1.746533852815628\n","Epoch time -----  0.875777006149292  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6290372371673585\n","Validation loss:  1.743423506617546\n","Epoch time -----  0.8743894100189209  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6237170219421386\n","Validation loss:  1.7394746452569962\n","Epoch time -----  0.8723173141479492  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6146762132644654\n","Validation loss:  1.7369772881269454\n","Epoch time -----  0.8436729907989502  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6069329619407653\n","Validation loss:  1.7376207441091538\n","Epoch time -----  0.699317216873169  sec\n","Epoch:  9\n","Train loss:  1.5993929266929627\n","Validation loss:  1.7329754799604415\n","Epoch time -----  0.7030003070831299  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5960108041763306\n","Validation loss:  1.7306880980730057\n","Epoch time -----  0.6618995666503906  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5896907687187194\n","Validation loss:  1.730530172586441\n","Epoch time -----  0.6627147197723389  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  74.5\n","Round:  7\n","Using entropy sampling on  39396  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  692\n","Training samples:  692\n","Training\n","Epoch:  1\n","Train loss:  1.663561680100181\n","Validation loss:  1.7241718918085098\n","Epoch time -----  0.907181978225708  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6430181373249402\n","Validation loss:  1.724390184879303\n","Epoch time -----  0.9045336246490479  sec\n","Epoch:  3\n","Train loss:  1.6325328024950894\n","Validation loss:  1.7197113305330276\n","Epoch time -----  0.8817138671875  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6211543083190918\n","Validation loss:  1.7159665793180465\n","Epoch time -----  0.9183225631713867  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6143560734662143\n","Validation loss:  1.7160698086023332\n","Epoch time -----  0.8969252109527588  sec\n","Epoch:  6\n","Train loss:  1.6087873740629717\n","Validation loss:  1.7127727538347244\n","Epoch time -----  0.8015830516815186  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5995369932868264\n","Validation loss:  1.7093640834093093\n","Epoch time -----  0.6215572357177734  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.595394416288896\n","Validation loss:  1.7109161823987962\n","Epoch time -----  0.7949409484863281  sec\n","Epoch:  9\n","Train loss:  1.5889991088347002\n","Validation loss:  1.7062652200460433\n","Epoch time -----  0.7664413452148438  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5834855816581033\n","Validation loss:  1.7061509549617768\n","Epoch time -----  0.8918042182922363  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5808954564007847\n","Validation loss:  1.7057806819677352\n","Epoch time -----  0.8995370864868164  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  78.01\n","Round:  8\n","Using entropy sampling on  39308  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  111  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  782\n","Training samples:  782\n","Training\n","Epoch:  1\n","Train loss:  1.6543614680950458\n","Validation loss:  1.7013509452342988\n","Epoch time -----  0.8649311065673828  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6339761477250319\n","Validation loss:  1.6976937383413315\n","Epoch time -----  0.6666309833526611  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.625110048514146\n","Validation loss:  1.6943031013011933\n","Epoch time -----  0.6895139217376709  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6208044107143695\n","Validation loss:  1.6935693264007567\n","Epoch time -----  0.6315944194793701  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6137664776581984\n","Validation loss:  1.6913866072893142\n","Epoch time -----  0.6719942092895508  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.611290115576524\n","Validation loss:  1.688134491443634\n","Epoch time -----  0.6348040103912354  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5985396641951342\n","Validation loss:  1.6878139972686768\n","Epoch time -----  0.6524999141693115  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.591081637602586\n","Validation loss:  1.6855643659830093\n","Epoch time -----  0.62727952003479  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.589344363946181\n","Validation loss:  1.6839770764112472\n","Epoch time -----  0.6465089321136475  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5812307046009944\n","Validation loss:  1.6845060974359511\n","Epoch time -----  0.6653201580047607  sec\n","Epoch:  11\n","Train loss:  1.5788247676996083\n","Validation loss:  1.6832657992839812\n","Epoch time -----  0.6867258548736572  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.76\n","Round:  9\n","Using entropy sampling on  39218  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  113  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  874\n","Training samples:  874\n","Training\n","Epoch:  1\n","Train loss:  1.64193993806839\n","Validation loss:  1.6809498757123946\n","Epoch time -----  0.6814332008361816  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6248340266091483\n","Validation loss:  1.6772372275590897\n","Epoch time -----  0.6264116764068604  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6162205168179102\n","Validation loss:  1.6755800515413284\n","Epoch time -----  0.6712961196899414  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6073871765817915\n","Validation loss:  1.6741419643163682\n","Epoch time -----  0.6710460186004639  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6025085364069258\n","Validation loss:  1.6731704264879226\n","Epoch time -----  0.6601428985595703  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5959692427090235\n","Validation loss:  1.6707428425550461\n","Epoch time -----  0.6280767917633057  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.592187225818634\n","Validation loss:  1.6701681286096572\n","Epoch time -----  0.6316747665405273  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5848775846617562\n","Validation loss:  1.6687991678714753\n","Epoch time -----  0.6685941219329834  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5821412205696106\n","Validation loss:  1.6678439736366273\n","Epoch time -----  0.636481523513794  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.577069776398795\n","Validation loss:  1.666362500190735\n","Epoch time -----  0.639603853225708  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.574974605015346\n","Validation loss:  1.665451142191887\n","Epoch time -----  0.6240568161010742  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.41\n","Round:  10\n","Using entropy sampling on  39126  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  114  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  966\n","Training samples:  966\n","Training\n","Epoch:  1\n","Train loss:  1.6388019546866417\n","Validation loss:  1.6633164554834365\n","Epoch time -----  0.7138993740081787  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6210949942469597\n","Validation loss:  1.6611782252788543\n","Epoch time -----  0.6614701747894287  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6262231767177582\n","Validation loss:  1.6604437738656999\n","Epoch time -----  0.832777738571167  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.603296510875225\n","Validation loss:  1.657490262389183\n","Epoch time -----  0.8577229976654053  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6040799245238304\n","Validation loss:  1.6574955105781555\n","Epoch time -----  0.9197876453399658  sec\n","Epoch:  6\n","Train loss:  1.5989889651536942\n","Validation loss:  1.6561382412910461\n","Epoch time -----  0.9129438400268555  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5932205989956856\n","Validation loss:  1.655631497502327\n","Epoch time -----  0.8229780197143555  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5847147703170776\n","Validation loss:  1.6538474172353745\n","Epoch time -----  0.7072865962982178  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5800186023116112\n","Validation loss:  1.6507410496473311\n","Epoch time -----  0.6811583042144775  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5793889835476875\n","Validation loss:  1.648997861146927\n","Epoch time -----  0.6537888050079346  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5789264366030693\n","Validation loss:  1.651743632555008\n","Epoch time -----  0.8210411071777344  sec\n","Epoch:  12\n","Train loss:  1.5737464651465416\n","Validation loss:  1.647714152932167\n","Epoch time -----  0.8497316837310791  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 1/10 [03:19<29:55, 199.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  80.95\n","Using CUDA\n","Test accuracy:  11.79\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.2987908124923706\n","Validation loss:  2.2833391427993774\n","Epoch time -----  0.8316550254821777  sec\n","Epoch:  2\n","Train loss:  2.206521987915039\n","Validation loss:  2.2036761820316313\n","Epoch time -----  0.8360779285430908  sec\n","Epoch:  3\n","Train loss:  1.9791894555091858\n","Validation loss:  2.162514740228653\n","Epoch time -----  0.66312575340271  sec\n","Epoch:  4\n","Train loss:  1.8441777229309082\n","Validation loss:  2.1601644575595857\n","Epoch time -----  0.6091041564941406  sec\n","Epoch:  5\n","Train loss:  1.8140610456466675\n","Validation loss:  2.1376162230968476\n","Epoch time -----  0.6281478404998779  sec\n","Epoch:  6\n","Train loss:  1.7631937861442566\n","Validation loss:  2.123612177371979\n","Epoch time -----  0.611274003982544  sec\n","Epoch:  7\n","Train loss:  1.7304165959358215\n","Validation loss:  2.1179202675819395\n","Epoch time -----  0.6255066394805908  sec\n","Epoch:  8\n","Train loss:  1.7010810375213623\n","Validation loss:  2.110113024711609\n","Epoch time -----  0.626746654510498  sec\n","Epoch:  9\n","Train loss:  1.6602885723114014\n","Validation loss:  2.1003273129463196\n","Epoch time -----  0.637462854385376  sec\n","Epoch:  10\n","Train loss:  1.6959313750267029\n","Validation loss:  2.093339830636978\n","Epoch time -----  0.614896297454834  sec\n","validation loss minimum, saving model\n","Test accuracy:  23.96\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  102  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  183\n","Training samples:  183\n","Training\n","Epoch:  1\n","Train loss:  1.926360289255778\n","Validation loss:  2.083998444676399\n","Epoch time -----  0.8539671897888184  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8812036514282227\n","Validation loss:  2.054731297492981\n","Epoch time -----  0.8299450874328613  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.812397559483846\n","Validation loss:  2.038995069265366\n","Epoch time -----  0.763178825378418  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.755689303080241\n","Validation loss:  2.038008689880371\n","Epoch time -----  0.6836230754852295  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7220817406972249\n","Validation loss:  2.032769373059273\n","Epoch time -----  0.7098486423492432  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6944035689036052\n","Validation loss:  2.024725267291069\n","Epoch time -----  0.6094624996185303  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6791778802871704\n","Validation loss:  2.018022373318672\n","Epoch time -----  0.6111688613891602  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6610547304153442\n","Validation loss:  2.0083919048309324\n","Epoch time -----  0.6275198459625244  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6371629238128662\n","Validation loss:  2.008162036538124\n","Epoch time -----  0.6218082904815674  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.630581021308899\n","Validation loss:  2.003441575169563\n","Epoch time -----  0.6527402400970459  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  37.32\n","Round:  2\n","Using entropy sampling on  39817  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  268\n","Training samples:  268\n","Training\n","Epoch:  1\n","Train loss:  1.7986191511154175\n","Validation loss:  1.9872233062982558\n","Epoch time -----  0.8981876373291016  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7608943939208985\n","Validation loss:  1.9727990716695785\n","Epoch time -----  0.905630350112915  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7559987783432007\n","Validation loss:  1.9603742361068726\n","Epoch time -----  0.8984169960021973  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7219857931137086\n","Validation loss:  1.9465864211320878\n","Epoch time -----  0.9122607707977295  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6759198188781739\n","Validation loss:  1.93651941716671\n","Epoch time -----  0.835899829864502  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6647407054901122\n","Validation loss:  1.929033064842224\n","Epoch time -----  0.6461374759674072  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6465917825698853\n","Validation loss:  1.9205780029296875\n","Epoch time -----  0.7471086978912354  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.654223895072937\n","Validation loss:  1.9119806855916976\n","Epoch time -----  0.684502363204956  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6129156351089478\n","Validation loss:  1.9035833805799485\n","Epoch time -----  0.8718719482421875  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6170568943023682\n","Validation loss:  1.8957883358001708\n","Epoch time -----  0.8587501049041748  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  51.81\n","Round:  3\n","Using entropy sampling on  39732  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  353\n","Training samples:  353\n","Training\n","Epoch:  1\n","Train loss:  1.7566162943840027\n","Validation loss:  1.8801088005304336\n","Epoch time -----  0.8586270809173584  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7221289078394573\n","Validation loss:  1.8690266609191895\n","Epoch time -----  0.8547379970550537  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7067704796791077\n","Validation loss:  1.8612427860498428\n","Epoch time -----  0.8643960952758789  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6731745998064678\n","Validation loss:  1.8578230440616608\n","Epoch time -----  0.846306562423706  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6608052452405293\n","Validation loss:  1.8536513477563858\n","Epoch time -----  0.8468039035797119  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6436828176180522\n","Validation loss:  1.8481490314006805\n","Epoch time -----  0.6237361431121826  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.626244028409322\n","Validation loss:  1.8427140146493912\n","Epoch time -----  0.6188313961029053  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.622839669386546\n","Validation loss:  1.837755274772644\n","Epoch time -----  0.6175529956817627  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6042198141415913\n","Validation loss:  1.836489549279213\n","Epoch time -----  0.6335091590881348  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5996606349945068\n","Validation loss:  1.833795315027237\n","Epoch time -----  0.615694522857666  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  64.06\n","Round:  4\n","Using entropy sampling on  39647  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  437\n","Training samples:  437\n","Training\n","Epoch:  1\n","Train loss:  1.7091890573501587\n","Validation loss:  1.8231000781059266\n","Epoch time -----  0.857682466506958  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6838526725769043\n","Validation loss:  1.8119333863258362\n","Epoch time -----  0.8693485260009766  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6666502101080758\n","Validation loss:  1.8052303493022919\n","Epoch time -----  0.8367748260498047  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6495531627110072\n","Validation loss:  1.8029818147420884\n","Epoch time -----  0.8626952171325684  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6365206582205636\n","Validation loss:  1.801198524236679\n","Epoch time -----  0.8755285739898682  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6252556528363908\n","Validation loss:  1.796074578166008\n","Epoch time -----  0.8594691753387451  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6159591845103674\n","Validation loss:  1.7909984022378922\n","Epoch time -----  0.7548127174377441  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6065307344709123\n","Validation loss:  1.7921988308429717\n","Epoch time -----  0.6053311824798584  sec\n","Epoch:  9\n","Train loss:  1.595728840146746\n","Validation loss:  1.7912947356700897\n","Epoch time -----  0.6410996913909912  sec\n","Epoch:  10\n","Train loss:  1.5883785826819283\n","Validation loss:  1.787330400943756\n","Epoch time -----  0.6336262226104736  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  74.34\n","Round:  5\n","Using entropy sampling on  39563  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  522\n","Training samples:  522\n","Training\n","Epoch:  1\n","Train loss:  1.6872474882337782\n","Validation loss:  1.7803614825010299\n","Epoch time -----  0.8327100276947021  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6656389368904962\n","Validation loss:  1.7775080919265747\n","Epoch time -----  0.8842542171478271  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6497491995493572\n","Validation loss:  1.7699251830577851\n","Epoch time -----  0.8724079132080078  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6424608760409884\n","Validation loss:  1.7698354214429854\n","Epoch time -----  0.8532865047454834  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6299504041671753\n","Validation loss:  1.7670897662639617\n","Epoch time -----  0.8705730438232422  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6275616486867268\n","Validation loss:  1.765587678551674\n","Epoch time -----  0.8611910343170166  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6159575117958918\n","Validation loss:  1.7638146966695785\n","Epoch time -----  0.8519723415374756  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6046179135640461\n","Validation loss:  1.7608685582876205\n","Epoch time -----  0.8675179481506348  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6026352908876207\n","Validation loss:  1.7574434667825698\n","Epoch time -----  0.6511204242706299  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5977331399917603\n","Validation loss:  1.7556470334529877\n","Epoch time -----  0.6104295253753662  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5851149823930528\n","Validation loss:  1.754296863079071\n","Epoch time -----  0.617311954498291  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  78.38\n","Round:  6\n","Using entropy sampling on  39478  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  607\n","Training samples:  607\n","Training\n","Epoch:  1\n","Train loss:  1.6792964935302734\n","Validation loss:  1.7494225054979324\n","Epoch time -----  0.8228230476379395  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6520002961158753\n","Validation loss:  1.7480169892311097\n","Epoch time -----  0.8639533519744873  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.644821786880493\n","Validation loss:  1.7426182180643082\n","Epoch time -----  0.882232666015625  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.632191526889801\n","Validation loss:  1.7399462521076203\n","Epoch time -----  0.8828721046447754  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.627135694026947\n","Validation loss:  1.738998717069626\n","Epoch time -----  0.8706750869750977  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6190929174423219\n","Validation loss:  1.7360740959644319\n","Epoch time -----  0.8661303520202637  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6097824335098267\n","Validation loss:  1.7336134135723114\n","Epoch time -----  0.8612501621246338  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6053689002990723\n","Validation loss:  1.7334294766187668\n","Epoch time -----  0.7630276679992676  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5963432431221007\n","Validation loss:  1.7307674318552018\n","Epoch time -----  0.6052184104919434  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5930579543113708\n","Validation loss:  1.7284278720617294\n","Epoch time -----  0.64164137840271  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5908600568771363\n","Validation loss:  1.7281302899122237\n","Epoch time -----  0.5782172679901123  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.08\n","Round:  7\n","Using entropy sampling on  39393  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  693\n","Training samples:  693\n","Training\n","Epoch:  1\n","Train loss:  1.6690015576102517\n","Validation loss:  1.723618346452713\n","Epoch time -----  0.6359608173370361  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6568194844506003\n","Validation loss:  1.7216447442770004\n","Epoch time -----  0.5849809646606445  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6417399861595847\n","Validation loss:  1.723361721634865\n","Epoch time -----  0.625251054763794  sec\n","Epoch:  4\n","Train loss:  1.6348194425756282\n","Validation loss:  1.7174334943294525\n","Epoch time -----  0.592571496963501  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6271253499117764\n","Validation loss:  1.7141613245010376\n","Epoch time -----  0.5782008171081543  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6182092753323642\n","Validation loss:  1.7140869885683059\n","Epoch time -----  0.6242456436157227  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6117250594225796\n","Validation loss:  1.7097412198781967\n","Epoch time -----  0.6137518882751465  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6071450060064143\n","Validation loss:  1.7071885645389557\n","Epoch time -----  0.6101574897766113  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6004485217007725\n","Validation loss:  1.7084978222846985\n","Epoch time -----  0.585852861404419  sec\n","Epoch:  10\n","Train loss:  1.595106753436002\n","Validation loss:  1.7052810966968537\n","Epoch time -----  0.591181755065918  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5916987224058672\n","Validation loss:  1.7042252004146576\n","Epoch time -----  0.5909004211425781  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.14\n","Round:  8\n","Using entropy sampling on  39307  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  780\n","Training samples:  780\n","Training\n","Epoch:  1\n","Train loss:  1.6564710507026086\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.703764048218727\n","Epoch time -----  0.6780459880828857  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6425165304770837\n","Validation loss:  1.6999065846204757\n","Epoch time -----  0.6078066825866699  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.631749831713163\n","Validation loss:  1.7015354812145234\n","Epoch time -----  0.5929183959960938  sec\n","Epoch:  4\n","Train loss:  1.621569009927603\n","Validation loss:  1.6962036609649658\n","Epoch time -----  0.5862038135528564  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6189021238913903\n","Validation loss:  1.6937704056501388\n","Epoch time -----  0.5932645797729492  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6106950319730318\n","Validation loss:  1.6928986877202987\n","Epoch time -----  0.5920422077178955  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6089912377871\n","Validation loss:  1.69099523127079\n","Epoch time -----  0.5881280899047852  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6048759405429547\n","Validation loss:  1.690426740050316\n","Epoch time -----  0.6264867782592773  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6014558627055242\n","Validation loss:  1.688736578822136\n","Epoch time -----  0.6098940372467041  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5895176300635705\n","Validation loss:  1.6896801441907883\n","Epoch time -----  0.6219973564147949  sec\n","Epoch:  11\n","Train loss:  1.588376155266395\n","Validation loss:  1.6878046959638595\n","Epoch time -----  0.5851092338562012  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.94\n","Round:  9\n","Using entropy sampling on  39220  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  868\n","Training samples:  868\n","Training\n","Epoch:  1\n","Train loss:  1.649874142238072\n","Validation loss:  1.6859380602836609\n","Epoch time -----  0.6689791679382324  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6375386033739363\n","Validation loss:  1.67904991209507\n","Epoch time -----  0.6426794528961182  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.627574895109449\n","Validation loss:  1.678991648554802\n","Epoch time -----  0.6469552516937256  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6170440571648734\n","Validation loss:  1.6763137221336364\n","Epoch time -----  0.65352463722229  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6095018642289298\n","Validation loss:  1.6765474796295166\n","Epoch time -----  0.6095583438873291  sec\n","Epoch:  6\n","Train loss:  1.608105148587908\n","Validation loss:  1.672969189286232\n","Epoch time -----  0.6011385917663574  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5987326502799988\n","Validation loss:  1.672006157040596\n","Epoch time -----  0.6189401149749756  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.594750804560525\n","Validation loss:  1.669609260559082\n","Epoch time -----  0.6088907718658447  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5916322554860796\n","Validation loss:  1.6690730094909667\n","Epoch time -----  0.6429836750030518  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5871184468269348\n","Validation loss:  1.6685343503952026\n","Epoch time -----  0.6486458778381348  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.582233965396881\n","Validation loss:  1.6659016013145447\n","Epoch time -----  0.6086416244506836  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  81.34\n","Round:  10\n","Using entropy sampling on  39132  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  111  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  957\n","Training samples:  957\n","Training\n","Epoch:  1\n","Train loss:  1.6399090051651002\n","Validation loss:  1.6650542706251144\n","Epoch time -----  0.6563482284545898  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.62617293993632\n","Validation loss:  1.6639234006404877\n","Epoch time -----  0.5887227058410645  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6171319961547852\n","Validation loss:  1.6605171352624892\n","Epoch time -----  0.5779101848602295  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6098966916402182\n","Validation loss:  1.6582545220851899\n","Epoch time -----  0.5999085903167725  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.603576366106669\n","Validation loss:  1.6582599580287933\n","Epoch time -----  0.599829912185669  sec\n","Epoch:  6\n","Train loss:  1.5983917872111002\n","Validation loss:  1.6570058792829514\n","Epoch time -----  0.5983846187591553  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5930077234903972\n","Validation loss:  1.6554279536008836\n","Epoch time -----  0.5950522422790527  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5889981190363567\n","Validation loss:  1.6541272133588791\n","Epoch time -----  0.607191801071167  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.585928169886271\n","Validation loss:  1.6555456578731538\n","Epoch time -----  0.5840370655059814  sec\n","Epoch:  10\n","Train loss:  1.5798333565394083\n","Validation loss:  1.6525462687015533\n","Epoch time -----  0.6354222297668457  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5774482885996501\n","Validation loss:  1.6523814439773559\n","Epoch time -----  0.6261844635009766  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.574612847963969\n","Validation loss:  1.6520506381988525\n","Epoch time -----  0.5903940200805664  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 2/10 [06:23<25:22, 190.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.39\n","Using CUDA\n","Test accuracy:  9.92\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.3058245182037354\n","Validation loss:  2.2861795246601107\n","Epoch time -----  0.6115498542785645  sec\n","Epoch:  2\n","Train loss:  2.2295737266540527\n","Validation loss:  2.18961301445961\n","Epoch time -----  0.5875744819641113  sec\n","Epoch:  3\n","Train loss:  1.9942139387130737\n","Validation loss:  2.157583546638489\n","Epoch time -----  0.5781917572021484  sec\n","Epoch:  4\n","Train loss:  1.8791825771331787\n","Validation loss:  2.1361038029193877\n","Epoch time -----  0.5881521701812744  sec\n","Epoch:  5\n","Train loss:  1.7826499342918396\n","Validation loss:  2.1285711884498597\n","Epoch time -----  0.5723991394042969  sec\n","Epoch:  6\n","Train loss:  1.7590650916099548\n","Validation loss:  2.1211666226387025\n","Epoch time -----  0.5931193828582764  sec\n","Epoch:  7\n","Train loss:  1.7505640983581543\n","Validation loss:  2.109620863199234\n","Epoch time -----  0.6381168365478516  sec\n","Epoch:  8\n","Train loss:  1.7174997925758362\n","Validation loss:  2.103603219985962\n","Epoch time -----  0.5935053825378418  sec\n","Epoch:  9\n","Train loss:  1.7004407048225403\n","Validation loss:  2.1000920236110687\n","Epoch time -----  0.5961389541625977  sec\n","Epoch:  10\n","Train loss:  1.718415379524231\n","Validation loss:  2.0961917281150817\n","Epoch time -----  0.5904316902160645  sec\n","Test accuracy:  31.23\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  184\n","Training samples:  184\n","Training\n","Epoch:  1\n","Train loss:  1.9606056213378906\n","Validation loss:  2.0886968612670898\n","Epoch time -----  0.6538169384002686  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.902698040008545\n","Validation loss:  2.072665536403656\n","Epoch time -----  0.5806999206542969  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8430592616399128\n","Validation loss:  2.063242721557617\n","Epoch time -----  0.5773811340332031  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8040331204732258\n","Validation loss:  2.052959567308426\n","Epoch time -----  0.5841307640075684  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.771699031194051\n","Validation loss:  2.043113535642624\n","Epoch time -----  0.5953216552734375  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7471453348795574\n","Validation loss:  2.0407509982585905\n","Epoch time -----  0.5865375995635986  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7236893971761067\n","Validation loss:  2.0426149755716323\n","Epoch time -----  0.596776008605957  sec\n","Epoch:  8\n","Train loss:  1.7069849173227947\n","Validation loss:  2.0410409539937975\n","Epoch time -----  0.5833277702331543  sec\n","Epoch:  9\n","Train loss:  1.6904360055923462\n","Validation loss:  2.037914681434631\n","Epoch time -----  0.603748083114624  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.681532899538676\n","Validation loss:  2.0340667247772215\n","Epoch time -----  0.5935413837432861  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  39.27\n","Round:  2\n","Using entropy sampling on  39816  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  268\n","Training samples:  268\n","Training\n","Epoch:  1\n","Train loss:  1.8557352066040038\n","Validation loss:  2.0211987107992173\n","Epoch time -----  0.6493964195251465  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8059267282485962\n","Validation loss:  2.008015087246895\n","Epoch time -----  0.5822234153747559  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.760772204399109\n","Validation loss:  1.9988662272691726\n","Epoch time -----  0.5861022472381592  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7768593788146974\n","Validation loss:  1.9848244398832322\n","Epoch time -----  0.627924919128418  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7137063026428223\n","Validation loss:  1.9760650604963304\n","Epoch time -----  0.5799598693847656  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7045030117034912\n","Validation loss:  1.970795711874962\n","Epoch time -----  0.6041474342346191  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.730257773399353\n","Validation loss:  1.9674064993858338\n","Epoch time -----  0.5867345333099365  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6690001010894775\n","Validation loss:  1.962567749619484\n","Epoch time -----  0.5844905376434326  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6784364223480224\n","Validation loss:  1.9529370456933974\n","Epoch time -----  0.5859594345092773  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6585184812545777\n","Validation loss:  1.9437612175941468\n","Epoch time -----  0.5847964286804199  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  56.75\n","Round:  3\n","Using entropy sampling on  39732  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  352\n","Training samples:  352\n","Training\n","Epoch:  1\n","Train loss:  1.802981158097585\n","Validation loss:  1.9357121139764786\n","Epoch time -----  0.6528129577636719  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.771369218826294\n","Validation loss:  1.936265704035759\n","Epoch time -----  0.5837419033050537  sec\n","Epoch:  3\n","Train loss:  1.7553610404332478\n","Validation loss:  1.9226626306772232\n","Epoch time -----  0.6165051460266113  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7347381711006165\n","Validation loss:  1.909206110239029\n","Epoch time -----  0.5821268558502197  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7155905564626057\n","Validation loss:  1.9002704262733459\n","Epoch time -----  0.5852737426757812  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7014129360516865\n","Validation loss:  1.8950861424207688\n","Epoch time -----  0.6231396198272705  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6769986748695374\n","Validation loss:  1.8907273352146148\n","Epoch time -----  0.6054060459136963  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6658908923467\n","Validation loss:  1.883322423696518\n","Epoch time -----  0.5744137763977051  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6454798380533855\n","Validation loss:  1.872791874408722\n","Epoch time -----  0.586881160736084  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6467423637708027\n","Validation loss:  1.8661642521619797\n","Epoch time -----  0.5840868949890137  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  65.72\n","Round:  4\n","Using entropy sampling on  39648  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  438\n","Training samples:  438\n","Training\n","Epoch:  1\n","Train loss:  1.7492478404726302\n","Validation loss:  1.8541346073150635\n","Epoch time -----  0.6549866199493408  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.726057035582406\n","Validation loss:  1.843503585457802\n","Epoch time -----  0.5908925533294678  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7094694205692835\n","Validation loss:  1.8349107831716538\n","Epoch time -----  0.5883810520172119  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6926366601671492\n","Validation loss:  1.8261530637741088\n","Epoch time -----  0.5983638763427734  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.674788543156215\n","Validation loss:  1.8187652200460434\n","Epoch time -----  0.5944526195526123  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.662281938961574\n","Validation loss:  1.8145186334848404\n","Epoch time -----  0.6378483772277832  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6511734213147844\n","Validation loss:  1.8075682193040847\n","Epoch time -----  0.6103720664978027  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6383529560906547\n","Validation loss:  1.8014757990837098\n","Epoch time -----  0.5883188247680664  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6304673297064645\n","Validation loss:  1.7968502551317216\n","Epoch time -----  0.6016557216644287  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6216744184494019\n","Validation loss:  1.7951023250818252\n","Epoch time -----  0.6121430397033691  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  68.46\n","Round:  5\n","Using entropy sampling on  39562  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  523\n","Training samples:  523\n","Training\n","Epoch:  1\n","Train loss:  1.7025899754630194\n","Validation loss:  1.7849802255630494\n","Epoch time -----  0.678539514541626  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6941876146528456\n","Validation loss:  1.7780727654695512\n","Epoch time -----  0.5922877788543701  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6734664969974093\n","Validation loss:  1.7790351986885071\n","Epoch time -----  0.6187655925750732  sec\n","Epoch:  4\n","Train loss:  1.6822619040807087\n","Validation loss:  1.7749359339475632\n","Epoch time -----  0.5779354572296143  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6492379109064739\n","Validation loss:  1.7685323357582092\n","Epoch time -----  0.5781192779541016  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.637232502301534\n","Validation loss:  1.7629877239465714\n","Epoch time -----  0.6329910755157471  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6412835253609552\n","Validation loss:  1.7590536415576934\n","Epoch time -----  0.6490697860717773  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.621689624256558\n","Validation loss:  1.758573552966118\n","Epoch time -----  0.6075344085693359  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6184762848748102\n","Validation loss:  1.754817932844162\n","Epoch time -----  0.6203539371490479  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6114278766844008\n","Validation loss:  1.7513111680746078\n","Epoch time -----  0.6471717357635498  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.6059280898835924\n","Validation loss:  1.7477957487106324\n","Epoch time -----  0.6658813953399658  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.2\n","Round:  6\n","Using entropy sampling on  39477  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  111  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  612\n","Training samples:  612\n","Training\n","Epoch:  1\n","Train loss:  1.6938504934310914\n","Validation loss:  1.7447928994894029\n","Epoch time -----  0.7014312744140625  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6731062054634094\n","Validation loss:  1.7404690206050872\n","Epoch time -----  0.6090514659881592  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.663618230819702\n","Validation loss:  1.7379809081554414\n","Epoch time -----  0.64290452003479  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6491922616958619\n","Validation loss:  1.7354330629110337\n","Epoch time -----  0.6182329654693604  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6382394075393676\n","Validation loss:  1.7318280607461929\n","Epoch time -----  0.6019644737243652  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6312599539756776\n","Validation loss:  1.72826087474823\n","Epoch time -----  0.6133244037628174  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6223060846328736\n","Validation loss:  1.727085617184639\n","Epoch time -----  0.630272626876831  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6147155284881591\n","Validation loss:  1.7252047806978226\n","Epoch time -----  0.659679651260376  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6068480849266051\n","Validation loss:  1.721648356318474\n","Epoch time -----  0.5928115844726562  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6008317470550537\n","Validation loss:  1.7211064577102662\n","Epoch time -----  0.5764997005462646  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5989046216011047\n","Validation loss:  1.7192051887512207\n","Epoch time -----  0.5907268524169922  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.9\n","Round:  7\n","Using entropy sampling on  39388  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  699\n","Training samples:  699\n","Training\n","Epoch:  1\n","Train loss:  1.6765027913180264\n","Validation loss:  1.7168925762176515\n","Epoch time -----  0.6598057746887207  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.658849228512157\n","Validation loss:  1.7119434893131256\n","Epoch time -----  0.6808571815490723  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6457192247564143\n","Validation loss:  1.7113883405923844\n","Epoch time -----  0.6324551105499268  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6364612579345703\n","Validation loss:  1.7074011415243149\n","Epoch time -----  0.6547515392303467  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6272529363632202\n","Validation loss:  1.7045796036720275\n","Epoch time -----  0.5804400444030762  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6188559857281772\n","Validation loss:  1.7042641550302506\n","Epoch time -----  0.5981814861297607  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6103048432957043\n","Validation loss:  1.700867822766304\n","Epoch time -----  0.642310380935669  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6043621084906838\n","Validation loss:  1.6979467391967773\n","Epoch time -----  0.6620359420776367  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5982838543978604\n","Validation loss:  1.6969230085611344\n","Epoch time -----  0.655829906463623  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5932203639637341\n","Validation loss:  1.6959785133600236\n","Epoch time -----  0.6163628101348877  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5894220200451938\n","Validation loss:  1.6933491289615632\n","Epoch time -----  0.6224839687347412  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  75.16\n","Round:  8\n","Using entropy sampling on  39301  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  787\n","Training samples:  787\n","Training\n","Epoch:  1\n","Train loss:  1.6566692223915687\n","Validation loss:  1.6947230666875839\n","Epoch time -----  0.6750156879425049  sec\n","Epoch:  2\n","Train loss:  1.6431972522002\n","Validation loss:  1.6910738795995712\n","Epoch time -----  0.6173162460327148  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6316196826788096\n","Validation loss:  1.6876608639955522\n","Epoch time -----  0.5989906787872314  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6229451527962317\n","Validation loss:  1.686189129948616\n","Epoch time -----  0.6015756130218506  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6204491670315082\n","Validation loss:  1.6836839884519577\n","Epoch time -----  0.606663703918457  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.606644162764916\n","Validation loss:  1.6831717193126678\n","Epoch time -----  0.5843114852905273  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.603032295520489\n","Validation loss:  1.6804327219724655\n","Epoch time -----  0.5717892646789551  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6004637388082652\n","Validation loss:  1.6801856219768525\n","Epoch time -----  0.5923831462860107  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5945114906017597\n","Validation loss:  1.6801310181617737\n","Epoch time -----  0.6144990921020508  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5938212688152606\n","Validation loss:  1.6779185712337494\n","Epoch time -----  0.5883486270904541  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5903646762554462\n","Validation loss:  1.677027827501297\n","Epoch time -----  0.5987510681152344  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  76.79\n","Round:  9\n","Using entropy sampling on  39213  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  876\n","Training samples:  876\n","Training\n","Epoch:  1\n","Train loss:  1.6527955446924483\n","Validation loss:  1.6772483140230179\n","Epoch time -----  0.6459972858428955  sec\n","Epoch:  2\n","Train loss:  1.6395453470093864\n","Validation loss:  1.6731676548719405\n","Epoch time -----  0.6345183849334717  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6259646926607405\n","Validation loss:  1.6727763205766677\n","Epoch time -----  0.6321842670440674  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6189467396054948\n","Validation loss:  1.6707678109407424\n","Epoch time -----  0.6075537204742432  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.611432501247951\n","Validation loss:  1.6681963801383972\n","Epoch time -----  0.6027965545654297  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.606238407748086\n","Validation loss:  1.665454736351967\n","Epoch time -----  0.5814874172210693  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6018020084926061\n","Validation loss:  1.6633684843778611\n","Epoch time -----  0.6420371532440186  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.595256779875074\n","Validation loss:  1.6632604151964188\n","Epoch time -----  0.6106083393096924  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.590627602168492\n","Validation loss:  1.6612538546323776\n","Epoch time -----  0.6412456035614014  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.586416815008436\n","Validation loss:  1.660575121641159\n","Epoch time -----  0.5838091373443604  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5818161538669042\n","Validation loss:  1.659137174487114\n","Epoch time -----  0.5894913673400879  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  76.65\n","Round:  10\n","Using entropy sampling on  39124  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  115  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  970\n","Training samples:  970\n","Training\n","Epoch:  1\n","Train loss:  1.6407449394464493\n","Validation loss:  1.6567774921655656\n","Epoch time -----  0.6589577198028564  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6301545947790146\n","Validation loss:  1.655457791686058\n","Epoch time -----  0.6091468334197998  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6246047392487526\n","Validation loss:  1.6536159813404083\n","Epoch time -----  0.6350858211517334  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.623663030564785\n","Validation loss:  1.6527614533901214\n","Epoch time -----  0.6004939079284668  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.613922856748104\n","Validation loss:  1.6491929322481156\n","Epoch time -----  0.6160125732421875  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5999545902013779\n","Validation loss:  1.646984252333641\n","Epoch time -----  0.5916492938995361  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.606615774333477\n","Validation loss:  1.6464185297489167\n","Epoch time -----  0.5906176567077637  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5939550772309303\n","Validation loss:  1.6448873043060304\n","Epoch time -----  0.5951366424560547  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5945143476128578\n","Validation loss:  1.6438526928424835\n","Epoch time -----  0.6074612140655518  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5862516984343529\n","Validation loss:  1.6426693588495254\n","Epoch time -----  0.5879700183868408  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.58449437469244\n","Validation loss:  1.6429084151983262\n","Epoch time -----  0.591090202331543  sec\n","Epoch:  12\n","Train loss:  1.5847798511385918\n","Validation loss:  1.6415542930364608\n","Epoch time -----  0.5888783931732178  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 3/10 [09:16<21:15, 182.28s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  79.53\n","Using CUDA\n","Test accuracy:  9.66\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.3070502281188965\n","Validation loss:  2.281998020410538\n","Epoch time -----  0.5804612636566162  sec\n","Epoch:  2\n","Train loss:  2.2032337188720703\n","Validation loss:  2.1848926663398744\n","Epoch time -----  0.5867593288421631  sec\n","Epoch:  3\n","Train loss:  1.9449998140335083\n","Validation loss:  2.139509356021881\n","Epoch time -----  0.5783817768096924  sec\n","Epoch:  4\n","Train loss:  1.8232247233390808\n","Validation loss:  2.122663623094559\n","Epoch time -----  0.5771653652191162  sec\n","Epoch:  5\n","Train loss:  1.732618272304535\n","Validation loss:  2.1269151747226713\n","Epoch time -----  0.5863926410675049  sec\n","Epoch:  6\n","Train loss:  1.720477283000946\n","Validation loss:  2.123707091808319\n","Epoch time -----  0.5853104591369629  sec\n","Epoch:  7\n","Train loss:  1.708978295326233\n","Validation loss:  2.1093588054180143\n","Epoch time -----  0.5822005271911621  sec\n","Epoch:  8\n","Train loss:  1.6994951963424683\n","Validation loss:  2.106276750564575\n","Epoch time -----  0.5827438831329346  sec\n","Epoch:  9\n","Train loss:  1.6822783946990967\n","Validation loss:  2.105770343542099\n","Epoch time -----  0.584343671798706  sec\n","Epoch:  10\n","Train loss:  1.6749765872955322\n","Validation loss:  2.112756532430649\n","Epoch time -----  0.5822708606719971  sec\n","Test accuracy:  26.31\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  183\n","Training samples:  183\n","Training\n","Epoch:  1\n","Train loss:  1.9442067543665569\n","Validation loss:  2.1050110042095183\n","Epoch time -----  0.6457593441009521  sec\n","Epoch:  2\n","Train loss:  1.898484468460083\n","Validation loss:  2.0795154571533203\n","Epoch time -----  0.6314864158630371  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.832242528597514\n","Validation loss:  2.0621902644634247\n","Epoch time -----  0.6621239185333252  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.778181791305542\n","Validation loss:  2.0495891898870466\n","Epoch time -----  0.5876076221466064  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7388621966044109\n","Validation loss:  2.03575414121151\n","Epoch time -----  0.5959765911102295  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7118521928787231\n","Validation loss:  2.0246135741472244\n","Epoch time -----  0.5900588035583496  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6906235218048096\n","Validation loss:  2.0110590636730192\n","Epoch time -----  0.579613208770752  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6721947193145752\n","Validation loss:  2.0007695317268372\n","Epoch time -----  0.5886778831481934  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6601978540420532\n","Validation loss:  1.9942340493202209\n","Epoch time -----  0.5863308906555176  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.651039719581604\n","Validation loss:  1.9891183376312256\n","Epoch time -----  0.639859676361084  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  45.07\n","Round:  2\n","Using entropy sampling on  39817  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  266\n","Training samples:  266\n","Training\n","Epoch:  1\n","Train loss:  1.8709336757659911\n","Validation loss:  1.983269813656807\n","Epoch time -----  0.6406149864196777  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8178237199783325\n","Validation loss:  1.9599100351333618\n","Epoch time -----  0.5997200012207031  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7311847448348998\n","Validation loss:  1.9480333834886552\n","Epoch time -----  0.6074120998382568  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.722910761833191\n","Validation loss:  1.9396932870149612\n","Epoch time -----  0.583033561706543  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6751654148101807\n","Validation loss:  1.9334365129470825\n","Epoch time -----  0.5963914394378662  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6635602712631226\n","Validation loss:  1.9294866174459457\n","Epoch time -----  0.590653657913208  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6624913454055785\n","Validation loss:  1.9219053834676743\n","Epoch time -----  0.5781147480010986  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.616023802757263\n","Validation loss:  1.9130424290895462\n","Epoch time -----  0.6098341941833496  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6031204223632813\n","Validation loss:  1.9049334675073624\n","Epoch time -----  0.6275320053100586  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5992473363876343\n","Validation loss:  1.8986110627651214\n","Epoch time -----  0.6433968544006348  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  58.72\n","Round:  3\n","Using entropy sampling on  39734  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  351\n","Training samples:  351\n","Training\n","Epoch:  1\n","Train loss:  1.7515928149223328\n","Validation loss:  1.8886814415454865\n","Epoch time -----  0.6294384002685547  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7123390634854634\n","Validation loss:  1.8800546675920486\n","Epoch time -----  0.5723953247070312  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6897328694661458\n","Validation loss:  1.8686635464429855\n","Epoch time -----  0.5906281471252441  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6685794194539387\n","Validation loss:  1.8607718080282212\n","Epoch time -----  0.6755731105804443  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.660595178604126\n","Validation loss:  1.8546487748622895\n","Epoch time -----  0.5960955619812012  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6407531102498372\n","Validation loss:  1.848437848687172\n","Epoch time -----  0.5706162452697754  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6224365433057149\n","Validation loss:  1.8446231722831725\n","Epoch time -----  0.5790300369262695  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.616392453511556\n","Validation loss:  1.8440591871738434\n","Epoch time -----  0.5938994884490967  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.602764089902242\n","Validation loss:  1.8432623535394668\n","Epoch time -----  0.5915026664733887  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5948444406191509\n","Validation loss:  1.8411260813474655\n","Epoch time -----  0.6279544830322266  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  60.07\n","Round:  4\n","Using entropy sampling on  39649  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  439\n","Training samples:  439\n","Training\n","Epoch:  1\n","Train loss:  1.7159680468695504\n","Validation loss:  1.8306640088558197\n","Epoch time -----  0.6560146808624268  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6949105943952287\n","Validation loss:  1.8164827466011046\n","Epoch time -----  0.5837762355804443  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6740436213357108\n","Validation loss:  1.8096863001585006\n","Epoch time -----  0.5789337158203125  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6557513645717077\n","Validation loss:  1.8068515241146088\n","Epoch time -----  0.6279940605163574  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6427543674196516\n","Validation loss:  1.8029161125421524\n","Epoch time -----  0.5910046100616455  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6314336061477661\n","Validation loss:  1.798285299539566\n","Epoch time -----  0.5999441146850586  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6210988930293493\n","Validation loss:  1.7955100417137146\n","Epoch time -----  0.6169095039367676  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.611523117337908\n","Validation loss:  1.791139480471611\n","Epoch time -----  0.6645116806030273  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6030698674065726\n","Validation loss:  1.7885270208120345\n","Epoch time -----  0.585953950881958  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5953518492834908\n","Validation loss:  1.7833614885807036\n","Epoch time -----  0.6237485408782959  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.72\n","Round:  5\n","Using entropy sampling on  39561  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  524\n","Training samples:  524\n","Training\n","Epoch:  1\n","Train loss:  1.6944642199410334\n","Validation loss:  1.777497234940529\n","Epoch time -----  0.676668643951416  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6666320032543607\n","Validation loss:  1.768505299091339\n","Epoch time -----  0.6516208648681641  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6692802376217313\n","Validation loss:  1.7646589815616607\n","Epoch time -----  0.6110579967498779  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.651046143637763\n","Validation loss:  1.7592837542295456\n","Epoch time -----  0.5939137935638428  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6240281263987224\n","Validation loss:  1.7563735395669937\n","Epoch time -----  0.6245987415313721  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.620833436648051\n","Validation loss:  1.754865723848343\n","Epoch time -----  0.5846447944641113  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.622140712208218\n","Validation loss:  1.7528428971767425\n","Epoch time -----  0.6003932952880859  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.611524264017741\n","Validation loss:  1.7490919053554534\n","Epoch time -----  0.5911638736724854  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6046037409040663\n","Validation loss:  1.7470750272274018\n","Epoch time -----  0.6569645404815674  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5953353113598294\n","Validation loss:  1.7457726925611496\n","Epoch time -----  0.6048586368560791  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5928747256596882\n","Validation loss:  1.7452771604061126\n","Epoch time -----  0.6168069839477539  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  77.21\n","Round:  6\n","Using entropy sampling on  39476  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  608\n","Training samples:  608\n","Training\n","Epoch:  1\n","Train loss:  1.6697837352752685\n","Validation loss:  1.7402549535036087\n","Epoch time -----  0.6503188610076904  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6505051732063294\n","Validation loss:  1.7389784425497055\n","Epoch time -----  0.6146702766418457  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6381344079971314\n","Validation loss:  1.7355974465608597\n","Epoch time -----  0.6395189762115479  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6269217371940612\n","Validation loss:  1.7325899094343185\n","Epoch time -----  0.5792877674102783  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6175493597984314\n","Validation loss:  1.7311938643455504\n","Epoch time -----  0.5716357231140137  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6101443767547607\n","Validation loss:  1.7277703195810319\n","Epoch time -----  0.5817387104034424  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6016033291816711\n","Validation loss:  1.7233872056007384\n","Epoch time -----  0.5916121006011963  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5991375684738158\n","Validation loss:  1.7227918535470963\n","Epoch time -----  0.584465503692627  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5899445176124574\n","Validation loss:  1.7220471322536468\n","Epoch time -----  0.5790431499481201  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5811346054077149\n","Validation loss:  1.7202282518148422\n","Epoch time -----  0.6200649738311768  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5802728295326234\n","Validation loss:  1.718472856283188\n","Epoch time -----  0.6836152076721191  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.05\n","Round:  7\n","Using entropy sampling on  39392  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  695\n","Training samples:  695\n","Training\n","Epoch:  1\n","Train loss:  1.6553712649778887\n","Validation loss:  1.7148168981075287\n","Epoch time -----  0.6495921611785889  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.638503367250616\n","Validation loss:  1.708593499660492\n","Epoch time -----  0.6008601188659668  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6293436072089456\n","Validation loss:  1.7086659282445908\n","Epoch time -----  0.6085314750671387  sec\n","Epoch:  4\n","Train loss:  1.6195585944435813\n","Validation loss:  1.7061103731393814\n","Epoch time -----  0.6094207763671875  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6126422882080078\n","Validation loss:  1.7049275577068328\n","Epoch time -----  0.6157855987548828  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6053270535035566\n","Validation loss:  1.7021913319826125\n","Epoch time -----  0.6061422824859619  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5974338054656982\n","Validation loss:  1.6990285396575928\n","Epoch time -----  0.6286063194274902  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5943165800788186\n","Validation loss:  1.697891464829445\n","Epoch time -----  0.5855846405029297  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.586719274520874\n","Validation loss:  1.6954527467489242\n","Epoch time -----  0.5925178527832031  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5818675539710305\n","Validation loss:  1.694061243534088\n","Epoch time -----  0.6257362365722656  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.576829043301669\n","Validation loss:  1.6932581454515456\n","Epoch time -----  0.6476888656616211  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.1\n","Round:  8\n","Using entropy sampling on  39305  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  784\n","Training samples:  784\n","Training\n","Epoch:  1\n","Train loss:  1.6461813083061805\n","Validation loss:  1.69034081697464\n","Epoch time -----  0.6828403472900391  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6361719278188853\n","Validation loss:  1.6848133325576782\n","Epoch time -----  0.5881736278533936  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6249882991497333\n","Validation loss:  1.6833764910697937\n","Epoch time -----  0.5861082077026367  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6157560440210195\n","Validation loss:  1.682400295138359\n","Epoch time -----  0.5854532718658447  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.61835162456219\n","Validation loss:  1.6804222464561462\n","Epoch time -----  0.5860333442687988  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6134675190998957\n","Validation loss:  1.6805738896131515\n","Epoch time -----  0.5904300212860107  sec\n","Epoch:  7\n","Train loss:  1.5947015560590303\n","Validation loss:  1.679221373796463\n","Epoch time -----  0.5946345329284668  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5984342831831713\n","Validation loss:  1.6769431829452515\n","Epoch time -----  0.6098096370697021  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.587418482853816\n","Validation loss:  1.6759575039148331\n","Epoch time -----  0.6041126251220703  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5810980613415058\n","Validation loss:  1.6728879153728484\n","Epoch time -----  0.6214358806610107  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5822869539260864\n","Validation loss:  1.6714830279350281\n","Epoch time -----  0.6119976043701172  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.55\n","Round:  9\n","Using entropy sampling on  39216  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  112  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  874\n","Training samples:  874\n","Training\n","Epoch:  1\n","Train loss:  1.6428006035940987\n","Validation loss:  1.6724085628986358\n","Epoch time -----  0.6492304801940918  sec\n","Epoch:  2\n","Train loss:  1.6312641416277205\n","Validation loss:  1.6705835938453675\n","Epoch time -----  0.5850353240966797  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6200727479798454\n","Validation loss:  1.6686666369438172\n","Epoch time -----  0.6020019054412842  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6094042318207877\n","Validation loss:  1.6660201013088227\n","Epoch time -----  0.5914063453674316  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6049651333263941\n","Validation loss:  1.6634545028209686\n","Epoch time -----  0.590660810470581  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6002952456474304\n","Validation loss:  1.6632466048002243\n","Epoch time -----  0.6073267459869385  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.593841484614781\n","Validation loss:  1.6629273027181626\n","Epoch time -----  0.6878883838653564  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.589057309286935\n","Validation loss:  1.6604925781488418\n","Epoch time -----  0.6800632476806641  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5841774429593767\n","Validation loss:  1.6590023070573807\n","Epoch time -----  0.6152992248535156  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5800538403647286\n","Validation loss:  1.6587843716144561\n","Epoch time -----  0.6040987968444824  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5762268986020769\n","Validation loss:  1.6580978512763977\n","Epoch time -----  0.6069481372833252  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.94\n","Round:  10\n","Using entropy sampling on  39126  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  111  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  963\n","Training samples:  963\n","Training\n","Epoch:  1\n","Train loss:  1.629603773355484\n","Validation loss:  1.6588326871395112\n","Epoch time -----  0.6632928848266602  sec\n","Epoch:  2\n","Train loss:  1.63694666326046\n","Validation loss:  1.655774426460266\n","Epoch time -----  0.6142380237579346  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6153659522533417\n","Validation loss:  1.6561407923698426\n","Epoch time -----  0.588646650314331  sec\n","Epoch:  4\n","Train loss:  1.612576425075531\n","Validation loss:  1.6544262915849686\n","Epoch time -----  0.5920267105102539  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6016682609915733\n","Validation loss:  1.6547493040561676\n","Epoch time -----  0.5900557041168213  sec\n","Epoch:  6\n","Train loss:  1.5978111699223518\n","Validation loss:  1.6519315779209136\n","Epoch time -----  0.6177160739898682  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6083798930048943\n","Validation loss:  1.6526191920042037\n","Epoch time -----  0.611112117767334  sec\n","Epoch:  8\n","Train loss:  1.6001653149724007\n","Validation loss:  1.6549217104911804\n","Epoch time -----  0.6282467842102051  sec\n","Epoch:  9\n","Train loss:  1.6076895743608475\n","Validation loss:  1.6505329698324203\n","Epoch time -----  0.6027274131774902  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5823093056678772\n","Validation loss:  1.6497896522283555\n","Epoch time -----  0.6370537281036377  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5960833355784416\n","Validation loss:  1.6498138159513474\n","Epoch time -----  0.6452939510345459  sec\n","Epoch:  12\n","Train loss:  1.600507840514183\n","Validation loss:  1.6510448217391969\n","Epoch time -----  0.6776618957519531  sec\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 4/10 [12:07<17:48, 178.01s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.14\n","Using CUDA\n","Test accuracy:  9.46\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.3081560134887695\n","Validation loss:  2.285947823524475\n","Epoch time -----  0.6278228759765625  sec\n","Epoch:  2\n","Train loss:  2.1542330980300903\n","Validation loss:  2.1609918415546416\n","Epoch time -----  0.569617509841919  sec\n","Epoch:  3\n","Train loss:  1.8967922925949097\n","Validation loss:  2.137580132484436\n","Epoch time -----  0.5928521156311035  sec\n","Epoch:  4\n","Train loss:  1.8182806968688965\n","Validation loss:  2.111041581630707\n","Epoch time -----  0.570300817489624  sec\n","Epoch:  5\n","Train loss:  1.7578686475753784\n","Validation loss:  2.1026005327701567\n","Epoch time -----  0.5710413455963135  sec\n","Epoch:  6\n","Train loss:  1.691416084766388\n","Validation loss:  2.0961394190788267\n","Epoch time -----  0.5712473392486572  sec\n","Epoch:  7\n","Train loss:  1.6676135063171387\n","Validation loss:  2.0899915397167206\n","Epoch time -----  0.5694255828857422  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6201940178871155\n","Validation loss:  2.088605839014053\n","Epoch time -----  0.5799000263214111  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.617587685585022\n","Validation loss:  2.0869633316993714\n","Epoch time -----  0.5858380794525146  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6191327571868896\n","Validation loss:  2.083806258440018\n","Epoch time -----  0.6275811195373535  sec\n","validation loss minimum, saving model\n","Test accuracy:  23.23\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  101  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  181\n","Training samples:  181\n","Training\n","Epoch:  1\n","Train loss:  1.8928752342859905\n","Validation loss:  2.073322147130966\n","Epoch time -----  0.7175357341766357  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8493002653121948\n","Validation loss:  2.057139527797699\n","Epoch time -----  0.6142184734344482  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.790266791979472\n","Validation loss:  2.046051698923111\n","Epoch time -----  0.6144189834594727  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7465181350708008\n","Validation loss:  2.034526860713959\n","Epoch time -----  0.6162006855010986  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7133151292800903\n","Validation loss:  2.026940757036209\n","Epoch time -----  0.6179602146148682  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6865241527557373\n","Validation loss:  2.0226710051298142\n","Epoch time -----  0.6238551139831543  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6717191139856975\n","Validation loss:  2.0170942902565003\n","Epoch time -----  0.625591516494751  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6486318508783977\n","Validation loss:  2.015972101688385\n","Epoch time -----  0.6378200054168701  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.636095364888509\n","Validation loss:  2.0125528037548066\n","Epoch time -----  0.6508708000183105  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6262191931406658\n","Validation loss:  2.003896898031235\n","Epoch time -----  0.6048033237457275  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  53.7\n","Round:  2\n","Using entropy sampling on  39819  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  267\n","Training samples:  267\n","Training\n","Epoch:  1\n","Train loss:  1.7910956859588623\n","Validation loss:  1.990784838795662\n","Epoch time -----  0.7040598392486572  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7471246719360352\n","Validation loss:  1.979729589819908\n","Epoch time -----  0.6197845935821533  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7270005941390991\n","Validation loss:  1.9675047874450684\n","Epoch time -----  0.6293995380401611  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7008522987365722\n","Validation loss:  1.958346351981163\n","Epoch time -----  0.6404027938842773  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6951403856277465\n","Validation loss:  1.9517924785614014\n","Epoch time -----  0.6340198516845703  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6548884630203247\n","Validation loss:  1.945970356464386\n","Epoch time -----  0.6088943481445312  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.674261212348938\n","Validation loss:  1.9394934624433517\n","Epoch time -----  0.6013884544372559  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6678138017654418\n","Validation loss:  1.9346769452095032\n","Epoch time -----  0.6299536228179932  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.62227303981781\n","Validation loss:  1.9309824287891388\n","Epoch time -----  0.6644306182861328  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6397656440734862\n","Validation loss:  1.9289011716842652\n","Epoch time -----  0.6298050880432129  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  64.13\n","Round:  3\n","Using entropy sampling on  39733  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  351\n","Training samples:  351\n","Training\n","Epoch:  1\n","Train loss:  1.7680644989013672\n","Validation loss:  1.9214956313371658\n","Epoch time -----  0.7287368774414062  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.737338125705719\n","Validation loss:  1.9131426632404327\n","Epoch time -----  0.6064968109130859  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7211793065071106\n","Validation loss:  1.9044691622257233\n","Epoch time -----  0.6011433601379395  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.695016086101532\n","Validation loss:  1.8945184737443923\n","Epoch time -----  0.5991158485412598  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6792713205019634\n","Validation loss:  1.8857034951448441\n","Epoch time -----  0.6061503887176514  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6588193972905476\n","Validation loss:  1.880495634675026\n","Epoch time -----  0.6258869171142578  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6532614827156067\n","Validation loss:  1.8754688292741775\n","Epoch time -----  0.650001049041748  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6324889659881592\n","Validation loss:  1.8687564551830291\n","Epoch time -----  0.6272294521331787  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6191348036130269\n","Validation loss:  1.8640610247850418\n","Epoch time -----  0.6281135082244873  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6110777854919434\n","Validation loss:  1.8612043917179109\n","Epoch time -----  0.6326324939727783  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  66.16\n","Round:  4\n","Using entropy sampling on  39649  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  436\n","Training samples:  436\n","Training\n","Epoch:  1\n","Train loss:  1.728860855102539\n","Validation loss:  1.8565243929624557\n","Epoch time -----  0.6229557991027832  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7093594244548254\n","Validation loss:  1.8527518004179\n","Epoch time -----  0.5773205757141113  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.688331024987357\n","Validation loss:  1.8452643483877182\n","Epoch time -----  0.5811259746551514  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6712218863623483\n","Validation loss:  1.8380091220140458\n","Epoch time -----  0.5735194683074951  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6527364764894759\n","Validation loss:  1.8305778056383133\n","Epoch time -----  0.5708167552947998  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.640361683709281\n","Validation loss:  1.8239314883947373\n","Epoch time -----  0.573131799697876  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6320249693734306\n","Validation loss:  1.8208208709955216\n","Epoch time -----  0.5819442272186279  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6226633105959212\n","Validation loss:  1.8195833057165145\n","Epoch time -----  0.5802488327026367  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6143267495291573\n","Validation loss:  1.8133048683404922\n","Epoch time -----  0.5792236328125  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6050054345812117\n","Validation loss:  1.8087573915719986\n","Epoch time -----  0.5777077674865723  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.7\n","Round:  5\n","Using entropy sampling on  39564  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  523\n","Training samples:  523\n","Training\n","Epoch:  1\n","Train loss:  1.6990686787499323\n","Validation loss:  1.8041042238473892\n","Epoch time -----  0.6883857250213623  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.689923259947035\n","Validation loss:  1.8004359871149063\n","Epoch time -----  0.617382287979126  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.671818044450548\n","Validation loss:  1.7938389003276825\n","Epoch time -----  0.6102957725524902  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6440352731280856\n","Validation loss:  1.7924819707870483\n","Epoch time -----  0.661381721496582  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6398365762498643\n","Validation loss:  1.7887849897146224\n","Epoch time -----  0.6481504440307617  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6249438259336684\n","Validation loss:  1.7854796081781388\n","Epoch time -----  0.6183550357818604  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6241574817233615\n","Validation loss:  1.781888723373413\n","Epoch time -----  0.5915613174438477  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6168559127383761\n","Validation loss:  1.7778849273920059\n","Epoch time -----  0.5889630317687988  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.604475326008267\n","Validation loss:  1.7737074464559555\n","Epoch time -----  0.6165330410003662  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.603223376803928\n","Validation loss:  1.771995735168457\n","Epoch time -----  0.607548713684082  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.60424894756741\n","Validation loss:  1.7690651059150695\n","Epoch time -----  0.6326413154602051  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  77.36\n","Round:  6\n","Using entropy sampling on  39477  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  607\n","Training samples:  607\n","Training\n","Epoch:  1\n","Train loss:  1.6795614361763\n","Validation loss:  1.7598282128572464\n","Epoch time -----  0.6701993942260742  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.663569414615631\n","Validation loss:  1.7568508088588715\n","Epoch time -----  0.5979089736938477  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6426027417182922\n","Validation loss:  1.751695731282234\n","Epoch time -----  0.608386754989624  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6312679529190064\n","Validation loss:  1.7457985699176788\n","Epoch time -----  0.5942883491516113  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.619974684715271\n","Validation loss:  1.7423276335000992\n","Epoch time -----  0.5991270542144775  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6130225896835326\n","Validation loss:  1.73865065574646\n","Epoch time -----  0.6005148887634277  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6052587151527404\n","Validation loss:  1.7340926319360732\n","Epoch time -----  0.5874180793762207  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5985817193984986\n","Validation loss:  1.7334007263183593\n","Epoch time -----  0.6058342456817627  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5940800905227661\n","Validation loss:  1.7300728380680084\n","Epoch time -----  0.5986394882202148  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5863474369049073\n","Validation loss:  1.7273836404085159\n","Epoch time -----  0.5875918865203857  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5792063474655151\n","Validation loss:  1.7267293840646745\n","Epoch time -----  0.5906107425689697  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.0\n","Round:  7\n","Using entropy sampling on  39393  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  694\n","Training samples:  694\n","Training\n","Epoch:  1\n","Train loss:  1.6554791277105159\n","Validation loss:  1.722796168923378\n","Epoch time -----  0.6365628242492676  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.642906362360174\n","Validation loss:  1.7216550648212432\n","Epoch time -----  0.6075077056884766  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.62846040725708\n","Validation loss:  1.7158865958452225\n","Epoch time -----  0.586707592010498  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6193916580893777\n","Validation loss:  1.714759635925293\n","Epoch time -----  0.5857555866241455  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6108788793737239\n","Validation loss:  1.7138091564178466\n","Epoch time -----  0.6234760284423828  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6017351258884778\n","Validation loss:  1.7116287231445313\n","Epoch time -----  0.6092312335968018  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5984989404678345\n","Validation loss:  1.7092739373445511\n","Epoch time -----  0.6376028060913086  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5904741070487283\n","Validation loss:  1.7080452352762223\n","Epoch time -----  0.6241810321807861  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5862287933176213\n","Validation loss:  1.7072750955820084\n","Epoch time -----  0.6017634868621826  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5817297480323098\n","Validation loss:  1.705196076631546\n","Epoch time -----  0.5847063064575195  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5760673934763128\n","Validation loss:  1.7047740131616593\n","Epoch time -----  0.5866823196411133  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.65\n","Round:  8\n","Using entropy sampling on  39306  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  781\n","Training samples:  781\n","Training\n","Epoch:  1\n","Train loss:  1.6523669866415172\n","Validation loss:  1.7044897556304932\n","Epoch time -----  0.6454184055328369  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.626422616151663\n","Validation loss:  1.6977113693952561\n","Epoch time -----  0.6346461772918701  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6207559475531945\n","Validation loss:  1.700995311141014\n","Epoch time -----  0.6018829345703125  sec\n","Epoch:  4\n","Train loss:  1.6187704251362727\n","Validation loss:  1.6958219528198242\n","Epoch time -----  0.6188244819641113  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6071643095750074\n","Validation loss:  1.6927772283554077\n","Epoch time -----  0.6368122100830078  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6020679565576406\n","Validation loss:  1.6939772874116898\n","Epoch time -----  0.6299960613250732  sec\n","Epoch:  7\n","Train loss:  1.5924211190297053\n","Validation loss:  1.690045353770256\n","Epoch time -----  0.6495449542999268  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.590659838456374\n","Validation loss:  1.6878560304641723\n","Epoch time -----  0.5987081527709961  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5856655561006987\n","Validation loss:  1.6852287173271179\n","Epoch time -----  0.599740743637085  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5825318923363318\n","Validation loss:  1.6847253650426866\n","Epoch time -----  0.6224358081817627  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5752727893682628\n","Validation loss:  1.6844430804252624\n","Epoch time -----  0.6814610958099365  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.92\n","Round:  9\n","Using entropy sampling on  39219  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  869\n","Training samples:  869\n","Training\n","Epoch:  1\n","Train loss:  1.6426314456122262\n","Validation loss:  1.679189196228981\n","Epoch time -----  0.6373155117034912  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6299272264753069\n","Validation loss:  1.6769616842269897\n","Epoch time -----  0.624535322189331  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6220634153911047\n","Validation loss:  1.6768565684556962\n","Epoch time -----  0.5945191383361816  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6161347031593323\n","Validation loss:  1.6726401418447494\n","Epoch time -----  0.6003332138061523  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6071535519191198\n","Validation loss:  1.670701813697815\n","Epoch time -----  0.6051437854766846  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5980271867343359\n","Validation loss:  1.668695431947708\n","Epoch time -----  0.594679594039917  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5945313487734114\n","Validation loss:  1.667209905385971\n","Epoch time -----  0.6105194091796875  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5895177211080278\n","Validation loss:  1.6658951789140701\n","Epoch time -----  0.6750545501708984  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5879833102226257\n","Validation loss:  1.6646500527858734\n","Epoch time -----  0.634716272354126  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5811721937997\n","Validation loss:  1.6627898931503295\n","Epoch time -----  0.6358497142791748  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5769129395484924\n","Validation loss:  1.6617304146289826\n","Epoch time -----  0.6423177719116211  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.68\n","Round:  10\n","Using entropy sampling on  39131  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  957\n","Training samples:  957\n","Training\n","Epoch:  1\n","Train loss:  1.6357162555058797\n","Validation loss:  1.660309800505638\n","Epoch time -----  0.7424783706665039  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6199921210606893\n","Validation loss:  1.6580270648002624\n","Epoch time -----  0.6761627197265625  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.610361115137736\n","Validation loss:  1.6569391727447509\n","Epoch time -----  0.6283128261566162  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6040066480636597\n","Validation loss:  1.6552190124988555\n","Epoch time -----  0.629133939743042  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.5972004572550456\n","Validation loss:  1.6528706878423691\n","Epoch time -----  0.6659574508666992  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5920695225397745\n","Validation loss:  1.652103853225708\n","Epoch time -----  0.6642696857452393  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5879811604817708\n","Validation loss:  1.6504207134246827\n","Epoch time -----  0.6625199317932129  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5839253187179565\n","Validation loss:  1.6484687983989716\n","Epoch time -----  0.5981159210205078  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.579183038075765\n","Validation loss:  1.6488057583570481\n","Epoch time -----  0.6502742767333984  sec\n","Epoch:  10\n","Train loss:  1.574957251548767\n","Validation loss:  1.6473816722631454\n","Epoch time -----  0.6360151767730713  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.57252836227417\n","Validation loss:  1.6463199108839035\n","Epoch time -----  0.6621947288513184  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.5695501565933228\n","Validation loss:  1.646514192223549\n","Epoch time -----  0.650277853012085  sec\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 5/10 [15:02<14:44, 176.96s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.23\n","Using CUDA\n","Test accuracy:  10.49\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.3171788454055786\n","Validation loss:  2.296090066432953\n","Epoch time -----  0.6791279315948486  sec\n","Epoch:  2\n","Train loss:  2.237576723098755\n","Validation loss:  2.204558515548706\n","Epoch time -----  0.5743927955627441  sec\n","Epoch:  3\n","Train loss:  2.0038307905197144\n","Validation loss:  2.1512444376945496\n","Epoch time -----  0.5897383689880371  sec\n","Epoch:  4\n","Train loss:  1.8507581949234009\n","Validation loss:  2.129515027999878\n","Epoch time -----  0.5809249877929688  sec\n","Epoch:  5\n","Train loss:  1.8114060163497925\n","Validation loss:  2.1157580494880674\n","Epoch time -----  0.5772140026092529  sec\n","Epoch:  6\n","Train loss:  1.757733166217804\n","Validation loss:  2.1068145394325257\n","Epoch time -----  0.5840888023376465  sec\n","Epoch:  7\n","Train loss:  1.7161371111869812\n","Validation loss:  2.1029080927371977\n","Epoch time -----  0.5682411193847656  sec\n","Epoch:  8\n","Train loss:  1.7060424089431763\n","Validation loss:  2.105383449792862\n","Epoch time -----  0.5674684047698975  sec\n","Epoch:  9\n","Train loss:  1.7012904286384583\n","Validation loss:  2.104886716604233\n","Epoch time -----  0.5787968635559082  sec\n","Epoch:  10\n","Train loss:  1.661321222782135\n","Validation loss:  2.103731781244278\n","Epoch time -----  0.5694124698638916  sec\n","Test accuracy:  27.18\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  183\n","Training samples:  183\n","Training\n","Epoch:  1\n","Train loss:  1.9189428488413494\n","Validation loss:  2.0925924241542817\n","Epoch time -----  0.7132797241210938  sec\n","Epoch:  2\n","Train loss:  1.8621524572372437\n","Validation loss:  2.068929821252823\n","Epoch time -----  0.8124136924743652  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.811642090479533\n","Validation loss:  2.0626878678798675\n","Epoch time -----  0.8908727169036865  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7714813550313313\n","Validation loss:  2.048300528526306\n","Epoch time -----  0.9109573364257812  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7393685181935628\n","Validation loss:  2.0381919175386427\n","Epoch time -----  0.877964973449707  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7159325281778972\n","Validation loss:  2.0308797627687456\n","Epoch time -----  0.9029040336608887  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6941473484039307\n","Validation loss:  2.0212891578674315\n","Epoch time -----  0.891197919845581  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6754497289657593\n","Validation loss:  2.014316889643669\n","Epoch time -----  0.8676202297210693  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6608434518178303\n","Validation loss:  2.006082132458687\n","Epoch time -----  0.8854646682739258  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.643051028251648\n","Validation loss:  1.9996927320957183\n","Epoch time -----  0.8167548179626465  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  46.05\n","Round:  2\n","Using entropy sampling on  39817  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  269\n","Training samples:  269\n","Training\n","Epoch:  1\n","Train loss:  1.8314814805984496\n","Validation loss:  1.98818981051445\n","Epoch time -----  0.8308155536651611  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8178617477416992\n","Validation loss:  1.9730786114931107\n","Epoch time -----  0.8814232349395752  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7410544633865357\n","Validation loss:  1.96278877556324\n","Epoch time -----  0.8902857303619385  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7214186429977416\n","Validation loss:  1.9515372216701508\n","Epoch time -----  0.8883662223815918  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7318971633911133\n","Validation loss:  1.948016607761383\n","Epoch time -----  0.9168674945831299  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6903340816497803\n","Validation loss:  1.9327484875917436\n","Epoch time -----  0.9156992435455322  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6635100841522217\n","Validation loss:  1.9212238073349\n","Epoch time -----  0.9269399642944336  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6822962999343871\n","Validation loss:  1.9136303961277008\n","Epoch time -----  0.8881266117095947  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6353769063949586\n","Validation loss:  1.9094336837530137\n","Epoch time -----  0.8564708232879639  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6284108877182006\n","Validation loss:  1.9082709521055221\n","Epoch time -----  0.6728882789611816  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  61.33\n","Round:  3\n","Using entropy sampling on  39731  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  353\n","Training samples:  353\n","Training\n","Epoch:  1\n","Train loss:  1.7755559086799622\n","Validation loss:  1.8996408939361573\n","Epoch time -----  0.9056916236877441  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7363816897074382\n","Validation loss:  1.8920904099941254\n","Epoch time -----  0.8924779891967773  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7226637800534566\n","Validation loss:  1.8848487049341203\n","Epoch time -----  0.8907618522644043  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.694496989250183\n","Validation loss:  1.8735825359821319\n","Epoch time -----  0.9156413078308105  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6798569162686665\n","Validation loss:  1.8662537574768066\n","Epoch time -----  0.9156937599182129  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6671993732452393\n","Validation loss:  1.8582393407821656\n","Epoch time -----  0.8867928981781006  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.647420843442281\n","Validation loss:  1.8516796857118607\n","Epoch time -----  0.882204532623291  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.637883981068929\n","Validation loss:  1.8441956281661986\n","Epoch time -----  0.8413128852844238  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6213948130607605\n","Validation loss:  1.8359997510910033\n","Epoch time -----  0.6617844104766846  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6052372058232625\n","Validation loss:  1.8314703464508058\n","Epoch time -----  0.6625676155090332  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  69.08\n","Round:  4\n","Using entropy sampling on  39647  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  437\n","Training samples:  437\n","Training\n","Epoch:  1\n","Train loss:  1.7225146463939123\n","Validation loss:  1.8213894724845887\n","Epoch time -----  0.8587074279785156  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6994851997920446\n","Validation loss:  1.8154088497161864\n","Epoch time -----  0.8561208248138428  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6862611430031913\n","Validation loss:  1.8052482068538667\n","Epoch time -----  0.8495824337005615  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6666527135031564\n","Validation loss:  1.8013182669878005\n","Epoch time -----  0.8396375179290771  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6560213906424386\n","Validation loss:  1.8000133365392685\n","Epoch time -----  0.8598291873931885  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6429213626044137\n","Validation loss:  1.7958973973989487\n","Epoch time -----  0.8673179149627686  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.634574362209865\n","Validation loss:  1.7901069581508637\n","Epoch time -----  0.8517396450042725  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.624795981815883\n","Validation loss:  1.786392679810524\n","Epoch time -----  0.7213363647460938  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6159459522792272\n","Validation loss:  1.783991453051567\n","Epoch time -----  0.697777509689331  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.608287249292646\n","Validation loss:  1.7816799193620683\n","Epoch time -----  0.618950605392456  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  74.33\n","Round:  5\n","Using entropy sampling on  39563  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  523\n","Training samples:  523\n","Training\n","Epoch:  1\n","Train loss:  1.7070286141501532\n","Validation loss:  1.775295239686966\n","Epoch time -----  0.8774335384368896  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6906000110838149\n","Validation loss:  1.7709796249866485\n","Epoch time -----  0.8773913383483887  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.670575115415785\n","Validation loss:  1.7644319415092469\n","Epoch time -----  0.8583822250366211  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6553590430153742\n","Validation loss:  1.7600895196199418\n","Epoch time -----  0.8687026500701904  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6584686040878296\n","Validation loss:  1.755584141612053\n","Epoch time -----  0.8585646152496338  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6514382892184787\n","Validation loss:  1.7533067673444749\n","Epoch time -----  0.8917479515075684  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6446799569659762\n","Validation loss:  1.7535972833633422\n","Epoch time -----  0.8644328117370605  sec\n","Epoch:  8\n","Train loss:  1.6332325273089938\n","Validation loss:  1.7495708614587784\n","Epoch time -----  0.6372077465057373  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6294851700464885\n","Validation loss:  1.7465313911437987\n","Epoch time -----  0.6464481353759766  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6345128350787692\n","Validation loss:  1.744382816553116\n","Epoch time -----  0.645789623260498  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.6125060187445746\n","Validation loss:  1.7436481535434722\n","Epoch time -----  0.6503183841705322  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  77.11\n","Round:  6\n","Using entropy sampling on  39477  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  608\n","Training samples:  608\n","Training\n","Epoch:  1\n","Train loss:  1.6880464196205138\n","Validation loss:  1.7402006298303605\n","Epoch time -----  0.841423511505127  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.671193039417267\n","Validation loss:  1.7375487446784974\n","Epoch time -----  0.8434243202209473  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6604785323143005\n","Validation loss:  1.7353121399879456\n","Epoch time -----  0.8774087429046631  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.647541105747223\n","Validation loss:  1.7316444128751756\n","Epoch time -----  0.8728704452514648  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.63813658952713\n","Validation loss:  1.728476020693779\n","Epoch time -----  0.852940559387207  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6267147064208984\n","Validation loss:  1.7258646577596664\n","Epoch time -----  0.8530693054199219  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.618741774559021\n","Validation loss:  1.7239956080913543\n","Epoch time -----  0.8427033424377441  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6118111848831176\n","Validation loss:  1.7203659683465957\n","Epoch time -----  0.6625926494598389  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6064574241638183\n","Validation loss:  1.71846504509449\n","Epoch time -----  0.6468417644500732  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6013314247131347\n","Validation loss:  1.717746329307556\n","Epoch time -----  0.6132726669311523  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5992716193199157\n","Validation loss:  1.7158322930335999\n","Epoch time -----  0.6168713569641113  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.27\n","Round:  7\n","Using entropy sampling on  39392  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  113  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  700\n","Training samples:  700\n","Training\n","Epoch:  1\n","Train loss:  1.6738497885790737\n","Validation loss:  1.715415146946907\n","Epoch time -----  0.8615543842315674  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.662982084534385\n","Validation loss:  1.7090770721435546\n","Epoch time -----  0.8519411087036133  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6519386551596902\n","Validation loss:  1.7034702569246292\n","Epoch time -----  0.8803253173828125  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6411717588251287\n","Validation loss:  1.7006220191717147\n","Epoch time -----  0.8703434467315674  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6331529942425815\n","Validation loss:  1.700097566843033\n","Epoch time -----  0.8643708229064941  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6268306103619663\n","Validation loss:  1.6972075790166854\n","Epoch time -----  0.8836026191711426  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6189197301864624\n","Validation loss:  1.6952371090650558\n","Epoch time -----  0.8480565547943115  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.612036726691506\n","Validation loss:  1.6921448826789856\n","Epoch time -----  0.66286301612854  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.606556079604409\n","Validation loss:  1.6907360583543778\n","Epoch time -----  0.6319775581359863  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6013376387682827\n","Validation loss:  1.6888746827840806\n","Epoch time -----  0.6472618579864502  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5972082614898682\n","Validation loss:  1.6871319532394409\n","Epoch time -----  0.6336417198181152  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.2\n","Round:  8\n","Using entropy sampling on  39300  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  787\n","Training samples:  787\n","Training\n","Epoch:  1\n","Train loss:  1.664891976576585\n","Validation loss:  1.68464075922966\n","Epoch time -----  0.9013228416442871  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6489570232538076\n","Validation loss:  1.6815951645374299\n","Epoch time -----  0.8813493251800537  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6417554433529193\n","Validation loss:  1.6814752250909806\n","Epoch time -----  0.9021377563476562  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6343734906269953\n","Validation loss:  1.678026705980301\n","Epoch time -----  0.8748550415039062  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.623338827720055\n","Validation loss:  1.6782844871282578\n","Epoch time -----  0.8668043613433838  sec\n","Epoch:  6\n","Train loss:  1.6191732058158288\n","Validation loss:  1.6741812139749528\n","Epoch time -----  0.8941206932067871  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6141158158962543\n","Validation loss:  1.6740707993507384\n","Epoch time -----  0.8662407398223877  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6057083790118878\n","Validation loss:  1.6724339455366135\n","Epoch time -----  0.6581830978393555  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6037476246173565\n","Validation loss:  1.672005745768547\n","Epoch time -----  0.6237328052520752  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.596410687153156\n","Validation loss:  1.6711023092269897\n","Epoch time -----  0.6751830577850342  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5947402715682983\n","Validation loss:  1.670190167427063\n","Epoch time -----  0.6504712104797363  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.47\n","Round:  9\n","Using entropy sampling on  39213  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  874\n","Training samples:  874\n","Training\n","Epoch:  1\n","Train loss:  1.655992533479418\n","Validation loss:  1.6704686641693116\n","Epoch time -----  0.8698408603668213  sec\n","Epoch:  2\n","Train loss:  1.640981674194336\n","Validation loss:  1.663589134812355\n","Epoch time -----  0.8968660831451416  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6330353362219674\n","Validation loss:  1.6627067476511002\n","Epoch time -----  0.9025135040283203  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6242713332176208\n","Validation loss:  1.660018876194954\n","Epoch time -----  0.8975341320037842  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6165972011429923\n","Validation loss:  1.6581319361925124\n","Epoch time -----  0.8855006694793701  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6108307242393494\n","Validation loss:  1.657371163368225\n","Epoch time -----  0.8839216232299805  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6059509090014867\n","Validation loss:  1.6553913980722428\n","Epoch time -----  0.8707270622253418  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.601531216076442\n","Validation loss:  1.655515468120575\n","Epoch time -----  0.7427916526794434  sec\n","Epoch:  9\n","Train loss:  1.5958850043160575\n","Validation loss:  1.6539133399724961\n","Epoch time -----  0.6671628952026367  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5947193247931344\n","Validation loss:  1.6533603250980378\n","Epoch time -----  0.6448922157287598  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5881570322172982\n","Validation loss:  1.6530435770750045\n","Epoch time -----  0.6490073204040527  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  81.18\n","Round:  10\n","Using entropy sampling on  39126  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  114  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  966\n","Training samples:  966\n","Training\n","Epoch:  1\n","Train loss:  1.6593125760555267\n","Validation loss:  1.652823695540428\n","Epoch time -----  0.8938553333282471  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6381124779582024\n","Validation loss:  1.6490322589874267\n","Epoch time -----  0.9002132415771484  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6373960971832275\n","Validation loss:  1.6482860654592515\n","Epoch time -----  0.8988721370697021  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6209975257515907\n","Validation loss:  1.6490610361099243\n","Epoch time -----  0.8758289813995361  sec\n","Epoch:  5\n","Train loss:  1.626699909567833\n","Validation loss:  1.6470732063055038\n","Epoch time -----  0.8951945304870605  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6107975244522095\n","Validation loss:  1.6452590763568877\n","Epoch time -----  0.8744490146636963  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6075730174779892\n","Validation loss:  1.6445108294486999\n","Epoch time -----  0.8888165950775146  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6036236584186554\n","Validation loss:  1.6438731908798219\n","Epoch time -----  0.7863967418670654  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6015895083546638\n","Validation loss:  1.645151397585869\n","Epoch time -----  0.6785392761230469  sec\n","Epoch:  10\n","Train loss:  1.5933620929718018\n","Validation loss:  1.6446231424808502\n","Epoch time -----  0.645374059677124  sec\n","Epoch:  11\n","Train loss:  1.5877954587340355\n","Validation loss:  1.6438253432512284\n","Epoch time -----  0.6463375091552734  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.587167002260685\n","Validation loss:  1.6422081142663956\n","Epoch time -----  0.6169524192810059  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 6/10 [18:20<12:16, 184.09s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.54\n","Using CUDA\n","Test accuracy:  15.73\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.2966583967208862\n","Validation loss:  2.2632271707057954\n","Epoch time -----  0.6088461875915527  sec\n","Epoch:  2\n","Train loss:  2.1733345985412598\n","Validation loss:  2.1864781022071837\n","Epoch time -----  0.6148238182067871  sec\n","Epoch:  3\n","Train loss:  1.9044515490531921\n","Validation loss:  2.1627690315246584\n","Epoch time -----  0.5995907783508301  sec\n","Epoch:  4\n","Train loss:  1.8071823120117188\n","Validation loss:  2.149953192472458\n","Epoch time -----  0.589702844619751  sec\n","Epoch:  5\n","Train loss:  1.7698054909706116\n","Validation loss:  2.130163109302521\n","Epoch time -----  0.6120171546936035  sec\n","Epoch:  6\n","Train loss:  1.7270207405090332\n","Validation loss:  2.118321084976196\n","Epoch time -----  0.6148521900177002  sec\n","Epoch:  7\n","Train loss:  1.736784279346466\n","Validation loss:  2.111069220304489\n","Epoch time -----  0.6188476085662842  sec\n","Epoch:  8\n","Train loss:  1.7254148125648499\n","Validation loss:  2.1045680344104767\n","Epoch time -----  0.6158134937286377  sec\n","Epoch:  9\n","Train loss:  1.672714650630951\n","Validation loss:  2.103074449300766\n","Epoch time -----  0.5967741012573242  sec\n","Epoch:  10\n","Train loss:  1.674441635608673\n","Validation loss:  2.09936408996582\n","Epoch time -----  0.6078126430511475  sec\n","Test accuracy:  19.98\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  102  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  182\n","Training samples:  182\n","Training\n","Epoch:  1\n","Train loss:  1.9304815530776978\n","Validation loss:  2.090853637456894\n","Epoch time -----  0.7040081024169922  sec\n","Epoch:  2\n","Train loss:  1.8814795811971028\n","Validation loss:  2.0699809372425078\n","Epoch time -----  0.6932957172393799  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8021366596221924\n","Validation loss:  2.0598927974700927\n","Epoch time -----  0.6519818305969238  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7584089438120525\n","Validation loss:  2.0454267978668215\n","Epoch time -----  0.6214020252227783  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.734973947207133\n","Validation loss:  2.0304255694150926\n","Epoch time -----  0.6070313453674316  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.724051594734192\n","Validation loss:  2.0177474349737166\n","Epoch time -----  0.6329984664916992  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6985352039337158\n","Validation loss:  2.007942533493042\n","Epoch time -----  0.6846606731414795  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6760669151941936\n","Validation loss:  1.9974514067173004\n","Epoch time -----  0.7033243179321289  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6641021569569905\n","Validation loss:  1.986978641152382\n","Epoch time -----  0.6397385597229004  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.653676430384318\n","Validation loss:  1.979306223988533\n","Epoch time -----  0.602914571762085  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  37.6\n","Round:  2\n","Using entropy sampling on  39818  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  101  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  265\n","Training samples:  265\n","Training\n","Epoch:  1\n","Train loss:  1.8261341810226441\n","Validation loss:  1.974444928765297\n","Epoch time -----  0.6760048866271973  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7529545068740844\n","Validation loss:  1.9683000028133393\n","Epoch time -----  0.6897249221801758  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7333078384399414\n","Validation loss:  1.958157628774643\n","Epoch time -----  0.7077264785766602  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6865308284759521\n","Validation loss:  1.9492960184812547\n","Epoch time -----  0.6213638782501221  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6776307582855225\n","Validation loss:  1.9418404072523117\n","Epoch time -----  0.6502575874328613  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6946926355361938\n","Validation loss:  1.9371035426855088\n","Epoch time -----  0.6819088459014893  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6870129823684692\n","Validation loss:  1.9341306924819945\n","Epoch time -----  0.6935129165649414  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6538661003112793\n","Validation loss:  1.9325284659862518\n","Epoch time -----  0.6482999324798584  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6653254985809327\n","Validation loss:  1.9308410316705704\n","Epoch time -----  0.6317529678344727  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6080291032791139\n","Validation loss:  1.9244888335466386\n","Epoch time -----  0.633408784866333  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  50.14\n","Round:  3\n","Using entropy sampling on  39735  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  350\n","Training samples:  350\n","Training\n","Epoch:  1\n","Train loss:  1.772166093190511\n","Validation loss:  1.917054784297943\n","Epoch time -----  0.7002131938934326  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7377973993619282\n","Validation loss:  1.897769919037819\n","Epoch time -----  0.6915977001190186  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7067078153292339\n","Validation loss:  1.8880624949932099\n","Epoch time -----  0.7119498252868652  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6788028081258137\n","Validation loss:  1.8842209696769714\n","Epoch time -----  0.7153987884521484  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.667279342810313\n","Validation loss:  1.8854471385478973\n","Epoch time -----  0.6851508617401123  sec\n","Epoch:  6\n","Train loss:  1.6546064813931782\n","Validation loss:  1.8788926959037782\n","Epoch time -----  0.7178347110748291  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6423736015955608\n","Validation loss:  1.8723256528377532\n","Epoch time -----  0.7245867252349854  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6265623569488525\n","Validation loss:  1.8667802691459656\n","Epoch time -----  0.6827831268310547  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6126952767372131\n","Validation loss:  1.864353820681572\n","Epoch time -----  0.6643760204315186  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6030803322792053\n","Validation loss:  1.8634102314710617\n","Epoch time -----  0.6633126735687256  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  56.74\n","Round:  4\n","Using entropy sampling on  39650  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  435\n","Training samples:  435\n","Training\n","Epoch:  1\n","Train loss:  1.7221062864576067\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.8516057878732681\n","Epoch time -----  0.674868106842041  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7005339350019182\n","Validation loss:  1.8397304505109786\n","Epoch time -----  0.666053056716919  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6761775697980608\n","Validation loss:  1.8307573795318604\n","Epoch time -----  0.7101895809173584  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6546382733753748\n","Validation loss:  1.8277485251426697\n","Epoch time -----  0.6836268901824951  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.639374511582511\n","Validation loss:  1.8282164275646209\n","Epoch time -----  0.6632671356201172  sec\n","Epoch:  6\n","Train loss:  1.6306380544389998\n","Validation loss:  1.8235939890146255\n","Epoch time -----  0.6337318420410156  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6183693408966064\n","Validation loss:  1.816096693277359\n","Epoch time -----  0.6226143836975098  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6116910151072912\n","Validation loss:  1.81479914188385\n","Epoch time -----  0.6403520107269287  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.602231706891741\n","Validation loss:  1.8146399945020675\n","Epoch time -----  0.639927864074707  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.593806437083653\n","Validation loss:  1.8143558204174042\n","Epoch time -----  0.6244885921478271  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  61.84\n","Round:  5\n","Using entropy sampling on  39565  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  519\n","Training samples:  519\n","Training\n","Epoch:  1\n","Train loss:  1.697348157564799\n","Validation loss:  1.8010940641164779\n","Epoch time -----  0.6521012783050537  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6675258742438421\n","Validation loss:  1.7971892684698105\n","Epoch time -----  0.6568152904510498  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6721320019827948\n","Validation loss:  1.7906641393899918\n","Epoch time -----  0.637326717376709  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6361026499006484\n","Validation loss:  1.785983669757843\n","Epoch time -----  0.6257355213165283  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.643433107270135\n","Validation loss:  1.7841418176889419\n","Epoch time -----  0.6165792942047119  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6317281590567694\n","Validation loss:  1.780422306060791\n","Epoch time -----  0.6285436153411865  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6036814716127183\n","Validation loss:  1.7774791210889815\n","Epoch time -----  0.6488564014434814  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.615975711080763\n","Validation loss:  1.7745917677879333\n","Epoch time -----  0.5995597839355469  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6170864370134141\n","Validation loss:  1.7723677068948747\n","Epoch time -----  0.6071164608001709  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6133797036276922\n","Validation loss:  1.7718095034360886\n","Epoch time -----  0.6350119113922119  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5849443938997057\n","Validation loss:  1.767753142118454\n","Epoch time -----  0.6150851249694824  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  67.06\n","Round:  6\n","Using entropy sampling on  39481  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  607\n","Training samples:  607\n","Training\n","Epoch:  1\n","Train loss:  1.6797010540962218\n","Validation loss:  1.763350260257721\n","Epoch time -----  0.8588151931762695  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6562230587005615\n","Validation loss:  1.7578666746616363\n","Epoch time -----  0.6731276512145996  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6491690754890442\n","Validation loss:  1.7504203170537949\n","Epoch time -----  0.6646063327789307  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.634883463382721\n","Validation loss:  1.7481158375740051\n","Epoch time -----  0.6418218612670898  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6265356183052062\n","Validation loss:  1.7478036880493164\n","Epoch time -----  0.6020777225494385  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6223825097084046\n","Validation loss:  1.742844995856285\n","Epoch time -----  0.6291835308074951  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.619832491874695\n","Validation loss:  1.7409255653619766\n","Epoch time -----  0.5873308181762695  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6094269514083863\n","Validation loss:  1.739247789978981\n","Epoch time -----  0.5811097621917725  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6012782216072083\n","Validation loss:  1.7366001099348067\n","Epoch time -----  0.5945413112640381  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5999454975128173\n","Validation loss:  1.7332005053758621\n","Epoch time -----  0.5953221321105957  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5963006377220155\n","Validation loss:  1.7324434787034988\n","Epoch time -----  0.593005895614624  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  68.92\n","Round:  7\n","Using entropy sampling on  39393  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n"]},{"name":"stdout","output_type":"stream","text":["Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  693\n","Training samples:  693\n","Training\n","Epoch:  1\n","Train loss:  1.6712748245759443\n","Validation loss:  1.7353331953287126\n","Epoch time -----  0.8298397064208984  sec\n","Epoch:  2\n","Train loss:  1.6540664000944658\n","Validation loss:  1.7251768380403518\n","Epoch time -----  0.7781896591186523  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6448620341040872\n","Validation loss:  1.721275880932808\n","Epoch time -----  0.6009080410003662  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6328781192952937\n","Validation loss:  1.7185786753892898\n","Epoch time -----  0.6342008113861084  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6246691400354558\n","Validation loss:  1.7142134428024292\n","Epoch time -----  0.6023764610290527  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6185397126457908\n","Validation loss:  1.7118511974811554\n","Epoch time -----  0.6114740371704102  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6124645471572876\n","Validation loss:  1.7117369771003723\n","Epoch time -----  0.615380048751831  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6048894795504483\n","Validation loss:  1.7075350612401963\n","Epoch time -----  0.6287238597869873  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.599215193228288\n","Validation loss:  1.7075084567070007\n","Epoch time -----  0.6344425678253174  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5941228866577148\n","Validation loss:  1.7062590181827546\n","Epoch time -----  0.6499683856964111  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5917586846785112\n","Validation loss:  1.7042337149381637\n","Epoch time -----  0.6201393604278564  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  70.08\n","Round:  8\n","Using entropy sampling on  39307  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  781\n","Training samples:  781\n","Training\n","Epoch:  1\n","Train loss:  1.6527687769669752\n","Validation loss:  1.7054935544729233\n","Epoch time -----  0.8244318962097168  sec\n","Epoch:  2\n","Train loss:  1.6470661163330078\n","Validation loss:  1.6970906585454941\n","Epoch time -----  0.7232329845428467  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.631902997310345\n","Validation loss:  1.696772426366806\n","Epoch time -----  0.6832108497619629  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6273045539855957\n","Validation loss:  1.695711049437523\n","Epoch time -----  0.6317884922027588  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6244003956134503\n","Validation loss:  1.6910773813724518\n","Epoch time -----  0.6234090328216553  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6215143753932073\n","Validation loss:  1.6904714733362198\n","Epoch time -----  0.6185078620910645  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6046057114234338\n","Validation loss:  1.6929244726896286\n","Epoch time -----  0.6529483795166016  sec\n","Epoch:  8\n","Train loss:  1.6013999260388887\n","Validation loss:  1.6864803791046143\n","Epoch time -----  0.623650074005127  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6035617681650014\n","Validation loss:  1.68639075756073\n","Epoch time -----  0.6280667781829834  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5988428409282978\n","Validation loss:  1.6857685565948486\n","Epoch time -----  0.6136693954467773  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5934347739586463\n","Validation loss:  1.6855378061532975\n","Epoch time -----  0.6290912628173828  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  77.19\n","Round:  9\n","Using entropy sampling on  39219  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  112  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  872\n","Training samples:  872\n","Training\n","Epoch:  1\n","Train loss:  1.6474334938185555\n","Validation loss:  1.6792645394802093\n","Epoch time -----  0.7300269603729248  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6318472708974565\n","Validation loss:  1.6777050375938416\n","Epoch time -----  0.6490135192871094  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6226826821054732\n","Validation loss:  1.675462082028389\n","Epoch time -----  0.6382782459259033  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6135966607502528\n","Validation loss:  1.6739738881587982\n","Epoch time -----  0.6122927665710449  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.606990098953247\n","Validation loss:  1.6724933922290801\n","Epoch time -----  0.6198384761810303  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6000510113579887\n","Validation loss:  1.669881597161293\n","Epoch time -----  0.6174726486206055  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.592614574091775\n","Validation loss:  1.6679481893777848\n","Epoch time -----  0.6502439975738525  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.588788432734353\n","Validation loss:  1.6662732899188994\n","Epoch time -----  0.657428503036499  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5848989656993322\n","Validation loss:  1.6667338758707047\n","Epoch time -----  0.6160540580749512  sec\n","Epoch:  10\n","Train loss:  1.5813851271356856\n","Validation loss:  1.6653918415307998\n","Epoch time -----  0.6055185794830322  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5763644065175737\n","Validation loss:  1.6634871751070022\n","Epoch time -----  0.6222681999206543  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.89\n","Round:  10\n","Using entropy sampling on  39128  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  112  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  962\n","Training samples:  962\n","Training\n","Epoch:  1\n","Train loss:  1.6403093710541725\n","Validation loss:  1.6629885643720628\n","Epoch time -----  0.8637881278991699  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6597036570310593\n","Validation loss:  1.6626146227121352\n","Epoch time -----  0.6834390163421631  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6123237237334251\n","Validation loss:  1.6604826420545578\n","Epoch time -----  0.6542937755584717  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6328819766640663\n","Validation loss:  1.6601390719413758\n","Epoch time -----  0.6199617385864258  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6238159984350204\n","Validation loss:  1.6588863611221314\n","Epoch time -----  0.6422226428985596  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5937980562448502\n","Validation loss:  1.6583979904651642\n","Epoch time -----  0.6219391822814941  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.595563255250454\n","Validation loss:  1.65766499042511\n","Epoch time -----  0.6244776248931885  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.60540309548378\n","Validation loss:  1.6575875192880631\n","Epoch time -----  0.6346039772033691  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5893822610378265\n","Validation loss:  1.657527807354927\n","Epoch time -----  0.6383028030395508  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6024693474173546\n","Validation loss:  1.6579996764659881\n","Epoch time -----  0.6240010261535645  sec\n","Epoch:  11\n","Train loss:  1.5964492484927177\n","Validation loss:  1.6630102157592774\n","Epoch time -----  0.630896806716919  sec\n","Epoch:  12\n","Train loss:  1.586406148970127\n","Validation loss:  1.657440784573555\n","Epoch time -----  0.6226086616516113  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 7/10 [21:14<09:02, 180.82s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.35\n","Using CUDA\n","Test accuracy:  8.67\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.2930513620376587\n","Validation loss:  2.266644549369812\n","Epoch time -----  0.6523687839508057  sec\n","Epoch:  2\n","Train loss:  2.1761393547058105\n","Validation loss:  2.136807656288147\n","Epoch time -----  0.6192200183868408  sec\n","Epoch:  3\n","Train loss:  1.905000627040863\n","Validation loss:  2.1180154144763947\n","Epoch time -----  0.6149218082427979  sec\n","Epoch:  4\n","Train loss:  1.7839798331260681\n","Validation loss:  2.1100548267364503\n","Epoch time -----  0.6021244525909424  sec\n","Epoch:  5\n","Train loss:  1.7423566579818726\n","Validation loss:  2.109038013219833\n","Epoch time -----  0.6142823696136475  sec\n","Epoch:  6\n","Train loss:  1.7192026376724243\n","Validation loss:  2.105578202009201\n","Epoch time -----  0.6370222568511963  sec\n","Epoch:  7\n","Train loss:  1.680301547050476\n","Validation loss:  2.097954958677292\n","Epoch time -----  0.6503162384033203  sec\n","Epoch:  8\n","Train loss:  1.6620680093765259\n","Validation loss:  2.091243124008179\n","Epoch time -----  0.5951786041259766  sec\n","Epoch:  9\n","Train loss:  1.6552108526229858\n","Validation loss:  2.086147886514664\n","Epoch time -----  0.6184916496276855  sec\n","Epoch:  10\n","Train loss:  1.6568950414657593\n","Validation loss:  2.0774154245853422\n","Epoch time -----  0.6216163635253906  sec\n","validation loss minimum, saving model\n","Test accuracy:  36.21\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  183\n","Training samples:  183\n","Training\n","Epoch:  1\n","Train loss:  1.9048207998275757\n","Validation loss:  2.0682844281196595\n","Epoch time -----  0.8876304626464844  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.855535586675008\n","Validation loss:  2.05583518743515\n","Epoch time -----  0.8206195831298828  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8187822500864665\n","Validation loss:  2.039116156101227\n","Epoch time -----  0.5965883731842041  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.757150650024414\n","Validation loss:  2.0159567445516586\n","Epoch time -----  0.6139833927154541  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.732008457183838\n","Validation loss:  2.0038693487644195\n","Epoch time -----  0.6053473949432373  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7032221555709839\n","Validation loss:  1.9974285334348678\n","Epoch time -----  0.6075527667999268  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6781023740768433\n","Validation loss:  1.9940511673688888\n","Epoch time -----  0.6077051162719727  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6604957977930705\n","Validation loss:  1.9935200572013856\n","Epoch time -----  0.6179254055023193  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.646802266438802\n","Validation loss:  1.9910161972045899\n","Epoch time -----  0.6303203105926514  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.628312389055888\n","Validation loss:  1.981593668460846\n","Epoch time -----  0.6148109436035156  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  62.21\n","Round:  2\n","Using entropy sampling on  39817  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  267\n","Training samples:  267\n","Training\n","Epoch:  1\n","Train loss:  1.7904389142990111\n","Validation loss:  1.9713708937168122\n","Epoch time -----  0.861370325088501  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.766573715209961\n","Validation loss:  1.9555175691843032\n","Epoch time -----  0.6081082820892334  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7211724519729614\n","Validation loss:  1.9472139954566956\n","Epoch time -----  0.6320817470550537  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7000602245330811\n","Validation loss:  1.9401009500026702\n","Epoch time -----  0.6044833660125732  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6870846509933473\n","Validation loss:  1.936690291762352\n","Epoch time -----  0.6011998653411865  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.672841739654541\n","Validation loss:  1.9305987000465392\n","Epoch time -----  0.5976207256317139  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6494475841522216\n","Validation loss:  1.9255949169397355\n","Epoch time -----  0.6170358657836914  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.657197070121765\n","Validation loss:  1.9207172036170959\n","Epoch time -----  0.6394309997558594  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.638506531715393\n","Validation loss:  1.9181739002466203\n","Epoch time -----  0.6154365539550781  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6125377893447876\n","Validation loss:  1.9151378691196441\n","Epoch time -----  0.6182253360748291  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  65.08\n","Round:  3\n","Using entropy sampling on  39733  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  353\n","Training samples:  353\n","Training\n","Epoch:  1\n","Train loss:  1.7614210446675618\n","Validation loss:  1.907295173406601\n","Epoch time -----  0.707268238067627  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7369892795880635\n","Validation loss:  1.8994492322206498\n","Epoch time -----  0.6314959526062012  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7284791072209675\n","Validation loss:  1.896235743165016\n","Epoch time -----  0.6540267467498779  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.703592856725057\n","Validation loss:  1.8934608042240142\n","Epoch time -----  0.6669929027557373  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6905293067296345\n","Validation loss:  1.8888123989105225\n","Epoch time -----  0.6515505313873291  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6716242829958599\n","Validation loss:  1.8833133071660995\n","Epoch time -----  0.6422684192657471  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6574695110321045\n","Validation loss:  1.878714457154274\n","Epoch time -----  0.6387801170349121  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6427932977676392\n","Validation loss:  1.8726671159267425\n","Epoch time -----  0.6559579372406006  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6388178666432698\n","Validation loss:  1.866115802526474\n","Epoch time -----  0.6666412353515625  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6270134846369426\n","Validation loss:  1.861393392086029\n","Epoch time -----  0.6536319255828857  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.11\n","Round:  4\n","Using entropy sampling on  39647  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  438\n","Training samples:  438\n","Training\n","Epoch:  1\n","Train loss:  1.7371090820857458\n","Validation loss:  1.8597283840179444\n","Epoch time -----  0.7782013416290283  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7113111700330461\n","Validation loss:  1.8493106842041016\n","Epoch time -----  0.660292387008667  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6910369736807687\n","Validation loss:  1.8449204206466674\n","Epoch time -----  0.6171743869781494  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.670996972492763\n","Validation loss:  1.8394819676876069\n","Epoch time -----  0.622429370880127  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6533876827784948\n","Validation loss:  1.8336624085903168\n","Epoch time -----  0.6034812927246094  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.640737703868321\n","Validation loss:  1.8283045560121536\n","Epoch time -----  0.6232175827026367  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.628826311656407\n","Validation loss:  1.8256521701812745\n","Epoch time -----  0.6372780799865723  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6225668702806746\n","Validation loss:  1.8218306213617326\n","Epoch time -----  0.63535475730896  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6120206117630005\n","Validation loss:  1.8154395669698715\n","Epoch time -----  0.6095371246337891  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6026559557233537\n","Validation loss:  1.8121088773012162\n","Epoch time -----  0.6077275276184082  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.89\n","Round:  5\n","Using entropy sampling on  39562  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  526\n","Training samples:  526\n","Training\n","Epoch:  1\n","Train loss:  1.7103678915235732\n","Validation loss:  1.805487075448036\n","Epoch time -----  0.8708407878875732  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6754951609505548\n","Validation loss:  1.7978140890598298\n","Epoch time -----  0.7140684127807617  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.681488659646776\n","Validation loss:  1.7924854844808578\n","Epoch time -----  0.6488559246063232  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6517556111017864\n","Validation loss:  1.7861626207828523\n","Epoch time -----  0.6124372482299805  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6408063173294067\n","Validation loss:  1.7782240480184555\n","Epoch time -----  0.6619806289672852  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6282920704947577\n","Validation loss:  1.7725084990262985\n","Epoch time -----  0.624293327331543  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6279930538601346\n","Validation loss:  1.7692422270774841\n","Epoch time -----  0.6174945831298828  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.635333432091607\n","Validation loss:  1.7650276929140092\n","Epoch time -----  0.6232876777648926  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6117787096235487\n","Validation loss:  1.7625773966312408\n","Epoch time -----  0.6401374340057373  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6034238868289523\n","Validation loss:  1.7607535511255263\n","Epoch time -----  0.6058707237243652  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.6100961367289226\n","Validation loss:  1.7601495504379272\n","Epoch time -----  0.6135373115539551  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  75.6\n","Round:  6\n","Using entropy sampling on  39474  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  611\n","Training samples:  611\n","Training\n","Epoch:  1\n","Train loss:  1.679836916923523\n","Validation loss:  1.753334641456604\n","Epoch time -----  0.8518893718719482  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.668751096725464\n","Validation loss:  1.7489742428064345\n","Epoch time -----  0.6610109806060791  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6527587532997132\n","Validation loss:  1.747557657957077\n","Epoch time -----  0.6549360752105713  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.64241863489151\n","Validation loss:  1.7441759288311005\n","Epoch time -----  0.626539945602417  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.632383644580841\n","Validation loss:  1.7417924612760545\n","Epoch time -----  0.6121945381164551  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6278305411338807\n","Validation loss:  1.7391291081905365\n","Epoch time -----  0.6049952507019043  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.61836838722229\n","Validation loss:  1.7362430393695831\n","Epoch time -----  0.6072373390197754  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6101720452308654\n","Validation loss:  1.73433355987072\n","Epoch time -----  0.6217501163482666  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6073642373085022\n","Validation loss:  1.7316592752933502\n","Epoch time -----  0.6363773345947266  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6014599442481994\n","Validation loss:  1.7296960771083831\n","Epoch time -----  0.6708941459655762  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5953707456588746\n","Validation loss:  1.7288889050483705\n","Epoch time -----  0.6154329776763916  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  78.34\n","Round:  7\n","Using entropy sampling on  39389  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  699\n","Training samples:  699\n","Training\n","Epoch:  1\n","Train loss:  1.6695390289480037\n","Validation loss:  1.725261178612709\n","Epoch time -----  0.8485515117645264  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6571690927852283\n","Validation loss:  1.7210696578025817\n","Epoch time -----  0.6927657127380371  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6441772851076992\n","Validation loss:  1.717413729429245\n","Epoch time -----  0.6597094535827637  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6364520029588179\n","Validation loss:  1.7145448952913285\n","Epoch time -----  0.6181490421295166  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6278149864890359\n","Validation loss:  1.7147503316402435\n","Epoch time -----  0.6089131832122803  sec\n","Epoch:  6\n","Train loss:  1.624262733892961\n","Validation loss:  1.7125176787376404\n","Epoch time -----  0.6259264945983887  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6150061975825916\n","Validation loss:  1.7115166902542114\n","Epoch time -----  0.6268665790557861  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6082344813780352\n","Validation loss:  1.7088341295719147\n","Epoch time -----  0.6324470043182373  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6023592948913574\n","Validation loss:  1.7066908031702042\n","Epoch time -----  0.633042573928833  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5981129949743098\n","Validation loss:  1.7061813950538636\n","Epoch time -----  0.6465854644775391  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5941448211669922\n","Validation loss:  1.7035884618759156\n","Epoch time -----  0.6150965690612793  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.55\n","Round:  8\n","Using entropy sampling on  39301  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  786\n","Training samples:  786\n","Training\n","Epoch:  1\n","Train loss:  1.6571466464262743\n","Validation loss:  1.7005444169044495\n","Epoch time -----  0.8525254726409912  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6434070605498095\n","Validation loss:  1.6949487060308457\n","Epoch time -----  0.7185113430023193  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6359929763353789\n","Validation loss:  1.6920861899852753\n","Epoch time -----  0.6321537494659424  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6300646525162916\n","Validation loss:  1.6902016639709472\n","Epoch time -----  0.6136305332183838  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6179801867558405\n","Validation loss:  1.687170797586441\n","Epoch time -----  0.6130225658416748  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6117162521068866\n","Validation loss:  1.6851213216781615\n","Epoch time -----  0.6425447463989258  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.61637282371521\n","Validation loss:  1.6838771849870682\n","Epoch time -----  0.6404461860656738  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6022302554203913\n","Validation loss:  1.681813332438469\n","Epoch time -----  0.6794989109039307  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.597266903290382\n","Validation loss:  1.6818666487932206\n","Epoch time -----  0.6393115520477295  sec\n","Epoch:  10\n","Train loss:  1.592647066483131\n","Validation loss:  1.6792243063449859\n","Epoch time -----  0.6340053081512451  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5868608493071337\n","Validation loss:  1.679822701215744\n","Epoch time -----  0.6326591968536377  sec\n","Testing\n","Test accuracy:  80.09\n","Round:  9\n","Using entropy sampling on  39214  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  873\n","Training samples:  873\n","Training\n","Epoch:  1\n","Train loss:  1.6539498993328638\n","Validation loss:  1.6819500476121902\n","Epoch time -----  0.8727500438690186  sec\n","Epoch:  2\n","Train loss:  1.634063414164952\n","Validation loss:  1.6756699860095978\n","Epoch time -----  0.7999238967895508  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6254524333136422\n","Validation loss:  1.6724479794502258\n","Epoch time -----  0.645043134689331  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6157762578555517\n","Validation loss:  1.6692204773426056\n","Epoch time -----  0.6534197330474854  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6099825671740942\n","Validation loss:  1.6675301849842072\n","Epoch time -----  0.6543974876403809  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6061527643884932\n","Validation loss:  1.666123193502426\n","Epoch time -----  0.6407101154327393  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6007558277675085\n","Validation loss:  1.6645832121372224\n","Epoch time -----  0.6404330730438232  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5946487614086695\n","Validation loss:  1.6645086228847503\n","Epoch time -----  0.6342191696166992  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5898302452904838\n","Validation loss:  1.6627650797367095\n","Epoch time -----  0.6233353614807129  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5876967396054948\n","Validation loss:  1.6626228541135788\n","Epoch time -----  0.6230576038360596  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5826121824128287\n","Validation loss:  1.6600958675146102\n","Epoch time -----  0.6368675231933594  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.73\n","Round:  10\n","Using entropy sampling on  39127  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  961\n","Training samples:  961\n","Training\n","Epoch:  1\n","Train loss:  1.6343681514263153\n","Validation loss:  1.6616973787546159\n","Epoch time -----  0.8717465400695801  sec\n","Epoch:  2\n","Train loss:  1.6541291177272797\n","Validation loss:  1.6589084893465043\n","Epoch time -----  0.7277488708496094  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6150740385055542\n","Validation loss:  1.6606227934360505\n","Epoch time -----  0.6460638046264648  sec\n","Epoch:  4\n","Train loss:  1.6259235590696335\n","Validation loss:  1.6589499473571778\n","Epoch time -----  0.6488173007965088  sec\n","Epoch:  5\n","Train loss:  1.6068708822131157\n","Validation loss:  1.659979873895645\n","Epoch time -----  0.6165521144866943  sec\n","Epoch:  6\n","Train loss:  1.6057628393173218\n","Validation loss:  1.6583141833543777\n","Epoch time -----  0.6654007434844971  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6021369323134422\n","Validation loss:  1.656878063082695\n","Epoch time -----  0.638789176940918  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.603180192410946\n","Validation loss:  1.6546299815177918\n","Epoch time -----  0.7022290229797363  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5978635624051094\n","Validation loss:  1.652981272339821\n","Epoch time -----  0.6696839332580566  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5903277322649956\n","Validation loss:  1.6539475172758102\n","Epoch time -----  0.6830811500549316  sec\n","Epoch:  11\n","Train loss:  1.5874790325760841\n","Validation loss:  1.6518456667661667\n","Epoch time -----  0.6468398571014404  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.584868036210537\n","Validation loss:  1.6502026855945586\n","Epoch time -----  0.6397180557250977  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 8/10 [24:11<05:58, 179.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.12\n","Using CUDA\n","Test accuracy:  11.57\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.31729257106781\n","Validation loss:  2.2815247237682343\n","Epoch time -----  0.6861891746520996  sec\n","Epoch:  2\n","Train loss:  2.2035423517227173\n","Validation loss:  2.1635378003120422\n","Epoch time -----  0.6333813667297363  sec\n","Epoch:  3\n","Train loss:  1.9271686673164368\n","Validation loss:  2.1198095798492433\n","Epoch time -----  0.6371395587921143  sec\n","Epoch:  4\n","Train loss:  1.8033119440078735\n","Validation loss:  2.1018195033073424\n","Epoch time -----  0.6142916679382324  sec\n","Epoch:  5\n","Train loss:  1.7761258482933044\n","Validation loss:  2.090744638442993\n","Epoch time -----  0.6136312484741211  sec\n","Epoch:  6\n","Train loss:  1.7378122806549072\n","Validation loss:  2.0803718268871307\n","Epoch time -----  0.6152355670928955  sec\n","Epoch:  7\n","Train loss:  1.6594260334968567\n","Validation loss:  2.074746197462082\n","Epoch time -----  0.6307210922241211  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.653550148010254\n","Validation loss:  2.072266471385956\n","Epoch time -----  0.6170933246612549  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6319395303726196\n","Validation loss:  2.073439872264862\n","Epoch time -----  0.6084530353546143  sec\n","Epoch:  10\n","Train loss:  1.6250659823417664\n","Validation loss:  2.075407999753952\n","Epoch time -----  0.6574609279632568  sec\n","Test accuracy:  24.16\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  101  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  181\n","Training samples:  181\n","Training\n","Epoch:  1\n","Train loss:  1.8965254227320354\n","Validation loss:  2.062933272123337\n","Epoch time -----  0.8467545509338379  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.852035681406657\n","Validation loss:  2.035125324130058\n","Epoch time -----  0.846519947052002  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.765625\n","Validation loss:  2.0167441815137863\n","Epoch time -----  0.7246534824371338  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.718657374382019\n","Validation loss:  1.9997894525527955\n","Epoch time -----  0.6223561763763428  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6831398010253906\n","Validation loss:  1.988245740532875\n","Epoch time -----  0.6249942779541016  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.654695192972819\n","Validation loss:  1.9820134550333024\n","Epoch time -----  0.6108748912811279  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6362478733062744\n","Validation loss:  1.9800139278173448\n","Epoch time -----  0.6036312580108643  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6197610298792522\n","Validation loss:  1.9758965402841568\n","Epoch time -----  0.6108999252319336  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6036818822224934\n","Validation loss:  1.9649014472961426\n","Epoch time -----  0.5970795154571533  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5965867439905803\n","Validation loss:  1.9556760996580125\n","Epoch time -----  0.6320574283599854  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  44.42\n","Round:  2\n","Using entropy sampling on  39819  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  265\n","Training samples:  265\n","Training\n","Epoch:  1\n","Train loss:  1.800410771369934\n","Validation loss:  1.9553037971258163\n","Epoch time -----  0.8946254253387451  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7858390808105469\n","Validation loss:  1.9472857415676117\n","Epoch time -----  0.8965597152709961  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.722822380065918\n","Validation loss:  1.9368713080883027\n","Epoch time -----  0.7483105659484863  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6567058324813844\n","Validation loss:  1.929168266057968\n","Epoch time -----  0.6668789386749268  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6439849138259888\n","Validation loss:  1.9216456949710845\n","Epoch time -----  0.6628468036651611  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6531902551651\n","Validation loss:  1.913342460989952\n","Epoch time -----  0.6642413139343262  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6591256380081176\n","Validation loss:  1.9083531379699707\n","Epoch time -----  0.6648375988006592  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6142659664154053\n","Validation loss:  1.901387944817543\n","Epoch time -----  0.6677696704864502  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6189808130264283\n","Validation loss:  1.896060448884964\n","Epoch time -----  0.663581132888794  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.60389244556427\n","Validation loss:  1.895992887020111\n","Epoch time -----  0.6831965446472168  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  47.69\n","Round:  3\n","Using entropy sampling on  39735  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  349\n","Training samples:  349\n","Training\n","Epoch:  1\n","Train loss:  1.7512876391410828\n","Validation loss:  1.8752834111452104\n","Epoch time -----  0.87131667137146  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7210118770599365\n","Validation loss:  1.8670845687389375\n","Epoch time -----  0.8152644634246826  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.697822670141856\n","Validation loss:  1.86526201069355\n","Epoch time -----  0.6094837188720703  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6770316362380981\n","Validation loss:  1.8615940749645232\n","Epoch time -----  0.6003179550170898  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6535441875457764\n","Validation loss:  1.853793454170227\n","Epoch time -----  0.60992431640625  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6362998485565186\n","Validation loss:  1.8474593937397004\n","Epoch time -----  0.6324901580810547  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6160470843315125\n","Validation loss:  1.8442662954330444\n","Epoch time -----  0.631629228591919  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.601166307926178\n","Validation loss:  1.8418732434511185\n","Epoch time -----  0.6128799915313721  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.59399418036143\n","Validation loss:  1.8395179748535155\n","Epoch time -----  0.6210999488830566  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5822721719741821\n","Validation loss:  1.8348060131072998\n","Epoch time -----  0.644874095916748  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  64.69\n","Round:  4\n","Using entropy sampling on  39651  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  436\n","Training samples:  436\n","Training\n","Epoch:  1\n","Train loss:  1.7013248716081892\n","Validation loss:  1.8213381260633468\n","Epoch time -----  0.8415608406066895  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6795680522918701\n","Validation loss:  1.8112174779176713\n","Epoch time -----  0.7441365718841553  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6559401069368636\n","Validation loss:  1.8025590509176255\n","Epoch time -----  0.6005051136016846  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6422984429768153\n","Validation loss:  1.798130175471306\n","Epoch time -----  0.6529979705810547  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6247984170913696\n","Validation loss:  1.7940635353326797\n","Epoch time -----  0.644855260848999  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6159599678856986\n","Validation loss:  1.7891246646642684\n","Epoch time -----  0.6592636108398438  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6039039918354578\n","Validation loss:  1.7843275189399719\n","Epoch time -----  0.6119885444641113  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5952764238630022\n","Validation loss:  1.7821282476186753\n","Epoch time -----  0.6247220039367676  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5866194111960275\n","Validation loss:  1.78101666867733\n","Epoch time -----  0.6211111545562744  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5815493379320418\n","Validation loss:  1.7802407801151277\n","Epoch time -----  0.5938293933868408  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.24\n","Round:  5\n","Using entropy sampling on  39564  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  524\n","Training samples:  524\n","Training\n","Epoch:  1\n","Train loss:  1.6889783011542425\n","Validation loss:  1.7749188721179963\n","Epoch time -----  0.850877046585083  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6885194778442383\n","Validation loss:  1.7732481807470322\n","Epoch time -----  0.7019960880279541  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6645474698808458\n","Validation loss:  1.7689615339040756\n","Epoch time -----  0.6150214672088623  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.645726548300849\n","Validation loss:  1.7660886943340302\n","Epoch time -----  0.6190249919891357  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6394653850131564\n","Validation loss:  1.763096034526825\n","Epoch time -----  0.64739990234375  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.631076865726047\n","Validation loss:  1.7571260899305343\n","Epoch time -----  0.6604437828063965  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6193017959594727\n","Validation loss:  1.7535029262304307\n","Epoch time -----  0.6250786781311035  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6158522897296481\n","Validation loss:  1.7526357084512711\n","Epoch time -----  0.674008846282959  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5961075623830159\n","Validation loss:  1.7514400362968445\n","Epoch time -----  0.6490299701690674  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5969615247514513\n","Validation loss:  1.752126121520996\n","Epoch time -----  0.6449155807495117  sec\n","Epoch:  11\n","Train loss:  1.5931647883521185\n","Validation loss:  1.7523889780044555\n","Epoch time -----  0.6166703701019287  sec\n","Testing\n","Test accuracy:  75.48\n","Round:  6\n","Using entropy sampling on  39476  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  610\n","Training samples:  610\n","Training\n","Epoch:  1\n","Train loss:  1.686321759223938\n","Validation loss:  1.7412331134080887\n","Epoch time -----  0.8649969100952148  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.665196692943573\n","Validation loss:  1.740455985069275\n","Epoch time -----  0.7606616020202637  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.648741567134857\n","Validation loss:  1.7322872072458266\n","Epoch time -----  0.6183476448059082  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6412204027175903\n","Validation loss:  1.7291898638010026\n","Epoch time -----  0.6274940967559814  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6302263736724854\n","Validation loss:  1.729745951294899\n","Epoch time -----  0.6256780624389648  sec\n","Epoch:  6\n","Train loss:  1.6249696731567382\n","Validation loss:  1.7272279649972915\n","Epoch time -----  0.622797966003418  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6202715396881104\n","Validation loss:  1.7219504237174987\n","Epoch time -----  0.6416385173797607  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6149456024169921\n","Validation loss:  1.721957042813301\n","Epoch time -----  0.6215901374816895  sec\n","Epoch:  9\n","Train loss:  1.6082207441329956\n","Validation loss:  1.7223163902759553\n","Epoch time -----  0.6403331756591797  sec\n","Epoch:  10\n","Train loss:  1.6020214438438416\n","Validation loss:  1.7170187830924988\n","Epoch time -----  0.6524965763092041  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5983170986175537\n","Validation loss:  1.7155843496322631\n","Epoch time -----  0.6713271141052246  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.02\n","Round:  7\n","Using entropy sampling on  39390  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  698\n","Training samples:  698\n","Training\n","Epoch:  1\n","Train loss:  1.671582362868569\n","Validation loss:  1.7124738901853562\n","Epoch time -----  0.8446993827819824  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.655485294081948\n","Validation loss:  1.7098409682512283\n","Epoch time -----  0.783980131149292  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6467433409257368\n","Validation loss:  1.7024862229824067\n","Epoch time -----  0.6306710243225098  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6368498802185059\n","Validation loss:  1.7041203081607819\n","Epoch time -----  0.6483147144317627  sec\n","Epoch:  5\n","Train loss:  1.6293363246050747\n","Validation loss:  1.7007076799869538\n","Epoch time -----  0.6136391162872314  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6227706562389026\n","Validation loss:  1.698331293463707\n","Epoch time -----  0.627333402633667  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6148757934570312\n","Validation loss:  1.6976563602685928\n","Epoch time -----  0.6455974578857422  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6082022190093994\n","Validation loss:  1.6945818603038787\n","Epoch time -----  0.643303394317627  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6040839173577048\n","Validation loss:  1.6932586640119554\n","Epoch time -----  0.62758469581604  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5981294133446433\n","Validation loss:  1.6926893442869186\n","Epoch time -----  0.6160271167755127  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5933975631540471\n","Validation loss:  1.6906985193490982\n","Epoch time -----  0.665067195892334  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.54\n","Round:  8\n","Using entropy sampling on  39302  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  787\n","Training samples:  787\n","Training\n","Epoch:  1\n","Train loss:  1.659985826565669\n","Validation loss:  1.6917187988758087\n","Epoch time -----  0.8637442588806152  sec\n","Epoch:  2\n","Train loss:  1.6427928301004262\n","Validation loss:  1.6874108374118806\n","Epoch time -----  0.7729907035827637  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6340203101818378\n","Validation loss:  1.6845133900642395\n","Epoch time -----  0.6540474891662598  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.625537606386038\n","Validation loss:  1.6833830535411836\n","Epoch time -----  0.6257979869842529  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6166978432581975\n","Validation loss:  1.6802103489637374\n","Epoch time -----  0.6160769462585449  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6080721708444448\n","Validation loss:  1.6782617181539536\n","Epoch time -----  0.6396758556365967  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.601443180671105\n","Validation loss:  1.6776458323001862\n","Epoch time -----  0.6408402919769287  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6014517637399526\n","Validation loss:  1.675860771536827\n","Epoch time -----  0.6239614486694336  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5944235966755793\n","Validation loss:  1.6736732572317123\n","Epoch time -----  0.6255934238433838  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5888363764836237\n","Validation loss:  1.6743326038122177\n","Epoch time -----  0.6419632434844971  sec\n","Epoch:  11\n","Train loss:  1.5815555865948017\n","Validation loss:  1.671110111474991\n","Epoch time -----  0.6207387447357178  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  80.52\n","Round:  9\n","Using entropy sampling on  39213  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  875\n","Training samples:  875\n","Training\n","Epoch:  1\n","Train loss:  1.6442921502249581\n","Validation loss:  1.6688096433877946\n","Epoch time -----  0.8704535961151123  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6301529152052743\n","Validation loss:  1.6654754668474196\n","Epoch time -----  0.834578275680542  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.620048897606986\n","Validation loss:  1.6622922658920287\n","Epoch time -----  0.6623930931091309  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6107132519994463\n","Validation loss:  1.6615609318017959\n","Epoch time -----  0.6294338703155518  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6047617878232683\n","Validation loss:  1.6608196169137954\n","Epoch time -----  0.6466121673583984  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.5979989681925093\n","Validation loss:  1.6588054567575454\n","Epoch time -----  0.6154398918151855  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5925372328077043\n","Validation loss:  1.657730233669281\n","Epoch time -----  0.6459710597991943  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5878378578594752\n","Validation loss:  1.6569157391786575\n","Epoch time -----  0.6922702789306641  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5849801812853133\n","Validation loss:  1.6552900612354278\n","Epoch time -----  0.640709400177002  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5792339188711983\n","Validation loss:  1.6548148155212403\n","Epoch time -----  0.6356260776519775  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5766031742095947\n","Validation loss:  1.6544479370117187\n","Epoch time -----  0.6219780445098877  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  81.04\n","Round:  10\n","Using entropy sampling on  39125  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  111  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  965\n","Training samples:  965\n","Training\n","Epoch:  1\n","Train loss:  1.629626415669918\n","Validation loss:  1.6528471857309341\n","Epoch time -----  0.8716297149658203  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6247943863272667\n","Validation loss:  1.64898561835289\n","Epoch time -----  0.8599905967712402  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6066324561834335\n","Validation loss:  1.6476385712623596\n","Epoch time -----  0.7020373344421387  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6088213324546814\n","Validation loss:  1.6458189338445663\n","Epoch time -----  0.7138497829437256  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6048944741487503\n","Validation loss:  1.6450249433517456\n","Epoch time -----  0.6324443817138672  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.59459438174963\n","Validation loss:  1.643621489405632\n","Epoch time -----  0.6425323486328125  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5881175845861435\n","Validation loss:  1.6443209439516067\n","Epoch time -----  0.6489419937133789  sec\n","Epoch:  8\n","Train loss:  1.5821949392557144\n","Validation loss:  1.6426399916410446\n","Epoch time -----  0.6439735889434814  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5835455060005188\n","Validation loss:  1.6429594963788987\n","Epoch time -----  0.6409835815429688  sec\n","Epoch:  10\n","Train loss:  1.5832049325108528\n","Validation loss:  1.6424164742231369\n","Epoch time -----  0.6281139850616455  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5729627832770348\n","Validation loss:  1.6409754276275634\n","Epoch time -----  0.6189737319946289  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.5771563202142715\n","Validation loss:  1.6408693969249726\n","Epoch time -----  0.6225781440734863  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 9/10 [27:06<02:58, 178.30s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  81.58\n","Using CUDA\n","Test accuracy:  13.43\n","Training samples:  100\n","Epoch:  1\n","Train loss:  2.2841618061065674\n","Validation loss:  2.2665407538414\n","Epoch time -----  0.6770596504211426  sec\n","Epoch:  2\n","Train loss:  2.2058494091033936\n","Validation loss:  2.1702765583992005\n","Epoch time -----  0.6638970375061035  sec\n","Epoch:  3\n","Train loss:  1.936400830745697\n","Validation loss:  2.1386038780212404\n","Epoch time -----  0.6214461326599121  sec\n","Epoch:  4\n","Train loss:  1.835340976715088\n","Validation loss:  2.135556834936142\n","Epoch time -----  0.6491169929504395  sec\n","Epoch:  5\n","Train loss:  1.760282814502716\n","Validation loss:  2.1322743475437163\n","Epoch time -----  0.6485521793365479  sec\n","Epoch:  6\n","Train loss:  1.7102113962173462\n","Validation loss:  2.124401068687439\n","Epoch time -----  0.6093928813934326  sec\n","Epoch:  7\n","Train loss:  1.6961978077888489\n","Validation loss:  2.1150315582752226\n","Epoch time -----  0.6412909030914307  sec\n","Epoch:  8\n","Train loss:  1.6426647305488586\n","Validation loss:  2.107729458808899\n","Epoch time -----  0.6644768714904785  sec\n","Epoch:  9\n","Train loss:  1.6595087051391602\n","Validation loss:  2.1040812909603117\n","Epoch time -----  0.6339123249053955  sec\n","Epoch:  10\n","Train loss:  1.6453987956047058\n","Validation loss:  2.1018772602081297\n","Epoch time -----  0.6303582191467285  sec\n","Test accuracy:  28.32\n","Round:  1\n","Using entropy sampling on  39900  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  126  points\n","Using loss sampling on  105  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  184\n","Training samples:  184\n","Training\n","Epoch:  1\n","Train loss:  1.9292210737864177\n","Validation loss:  2.097425085306168\n","Epoch time -----  0.8490157127380371  sec\n","Epoch:  2\n","Train loss:  1.8892699082692463\n","Validation loss:  2.077798080444336\n","Epoch time -----  0.8539786338806152  sec\n","Epoch:  3\n","Train loss:  1.8093786636988323\n","Validation loss:  2.0603948414325712\n","Epoch time -----  0.8443021774291992  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7596904436747234\n","Validation loss:  2.041564244031906\n","Epoch time -----  0.6364600658416748  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7196158170700073\n","Validation loss:  2.0279965162277223\n","Epoch time -----  0.6423473358154297  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.69576895236969\n","Validation loss:  2.017312839627266\n","Epoch time -----  0.6161510944366455  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6783459583918254\n","Validation loss:  2.0108280718326568\n","Epoch time -----  0.6138954162597656  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6566483577092488\n","Validation loss:  2.0093885153532027\n","Epoch time -----  0.6120555400848389  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6353377501169841\n","Validation loss:  2.0076015651226045\n","Epoch time -----  0.6475625038146973  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6232033570607503\n","Validation loss:  2.0020906001329424\n","Epoch time -----  0.6179049015045166  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  48.12\n","Round:  2\n","Using entropy sampling on  39816  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  127  points\n","Using loss sampling on  102  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  266\n","Training samples:  266\n","Training\n","Epoch:  1\n","Train loss:  1.803053379058838\n","Validation loss:  1.987620770931244\n","Epoch time -----  0.8754103183746338  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7689669609069825\n","Validation loss:  1.9661342591047286\n","Epoch time -----  0.8828234672546387  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7229406356811523\n","Validation loss:  1.9533020228147506\n","Epoch time -----  0.7485494613647461  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6942723035812377\n","Validation loss:  1.9455450296401977\n","Epoch time -----  0.6652159690856934  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6735395669937134\n","Validation loss:  1.9371533244848251\n","Epoch time -----  0.6653420925140381  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.672474980354309\n","Validation loss:  1.9285478472709656\n","Epoch time -----  0.6641418933868408  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6814872980117799\n","Validation loss:  1.9210540354251862\n","Epoch time -----  0.697979211807251  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.644307827949524\n","Validation loss:  1.916417160630226\n","Epoch time -----  0.6736631393432617  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6211152076721191\n","Validation loss:  1.9133921802043914\n","Epoch time -----  0.6558742523193359  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.621326994895935\n","Validation loss:  1.9105891168117524\n","Epoch time -----  0.6656639575958252  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  58.18\n","Round:  3\n","Using entropy sampling on  39734  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  128  points\n","Using loss sampling on  103  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  349\n","Training samples:  349\n","Training\n","Epoch:  1\n","Train loss:  1.7541143695513408\n","Validation loss:  1.908217316865921\n","Epoch time -----  0.8544621467590332  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7222410440444946\n","Validation loss:  1.9081119418144226\n","Epoch time -----  0.8578550815582275  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7085156440734863\n","Validation loss:  1.900550439953804\n","Epoch time -----  0.6557233333587646  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6848013003667195\n","Validation loss:  1.8924182295799254\n","Epoch time -----  0.6084940433502197  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6868009169896443\n","Validation loss:  1.8888816028833388\n","Epoch time -----  0.6457962989807129  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6724148988723755\n","Validation loss:  1.8912758708000184\n","Epoch time -----  0.6111681461334229  sec\n","Epoch:  7\n","Train loss:  1.6708574096361797\n","Validation loss:  1.8846149623394013\n","Epoch time -----  0.6340253353118896  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6516087849934895\n","Validation loss:  1.8762544453144074\n","Epoch time -----  0.6319444179534912  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6484169562657673\n","Validation loss:  1.8731527596712112\n","Epoch time -----  0.6129145622253418  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6281627813975017\n","Validation loss:  1.8723864078521728\n","Epoch time -----  0.6169400215148926  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  62.99\n","Round:  4\n","Using entropy sampling on  39651  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  130  points\n","Using loss sampling on  109  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  437\n","Training samples:  437\n","Training\n","Epoch:  1\n","Train loss:  1.7487664222717285\n","Validation loss:  1.866163158416748\n","Epoch time -----  0.862581729888916  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7252103090286255\n","Validation loss:  1.8589694112539292\n","Epoch time -----  0.8469252586364746  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7071642535073417\n","Validation loss:  1.8458495885133743\n","Epoch time -----  0.6185119152069092  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6834842307226998\n","Validation loss:  1.8361944407224655\n","Epoch time -----  0.6460325717926025  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6694116592407227\n","Validation loss:  1.8284882664680482\n","Epoch time -----  0.6155316829681396  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6570302758898054\n","Validation loss:  1.8210853964090348\n","Epoch time -----  0.62740159034729  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.646328330039978\n","Validation loss:  1.818226957321167\n","Epoch time -----  0.6483688354492188  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.635101352419172\n","Validation loss:  1.8155712485313416\n","Epoch time -----  0.6475613117218018  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6265930107661657\n","Validation loss:  1.808556032180786\n","Epoch time -----  0.6313881874084473  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.6193970612117223\n","Validation loss:  1.8047979563474654\n","Epoch time -----  0.6188647747039795  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.61\n","Round:  5\n","Using entropy sampling on  39563  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  131  points\n","Using loss sampling on  104  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  521\n","Training samples:  521\n","Training\n","Epoch:  1\n","Train loss:  1.7278528345955744\n","Validation loss:  1.8017082422971726\n","Epoch time -----  0.8550441265106201  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6784569422403972\n","Validation loss:  1.799723568558693\n","Epoch time -----  0.8055102825164795  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6776492728127375\n","Validation loss:  1.7903218865394592\n","Epoch time -----  0.6180124282836914  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6723657846450806\n","Validation loss:  1.7876311272382737\n","Epoch time -----  0.627061128616333  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6597007645501032\n","Validation loss:  1.783992099761963\n","Epoch time -----  0.6227130889892578  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.668880091773139\n","Validation loss:  1.7815677314996718\n","Epoch time -----  0.598823070526123  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6367543273501926\n","Validation loss:  1.7769028127193451\n","Epoch time -----  0.5981740951538086  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6318005323410034\n","Validation loss:  1.7734035521745681\n","Epoch time -----  0.6076719760894775  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6208214494917128\n","Validation loss:  1.7711767464876176\n","Epoch time -----  0.6157617568969727  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.611425174607171\n","Validation loss:  1.7698420017957688\n","Epoch time -----  0.6488423347473145  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.6080880297554865\n","Validation loss:  1.7680202901363373\n","Epoch time -----  0.6448953151702881  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  75.1\n","Round:  6\n","Using entropy sampling on  39479  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  132  points\n","Using loss sampling on  106  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  606\n","Training samples:  606\n","Training\n","Epoch:  1\n","Train loss:  1.6946279644966125\n","Validation loss:  1.7660897374153137\n","Epoch time -----  0.8423123359680176  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6766742587089538\n","Validation loss:  1.7562347114086152\n","Epoch time -----  0.835360050201416  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.658473014831543\n","Validation loss:  1.749946042895317\n","Epoch time -----  0.6295974254608154  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6425593018531799\n","Validation loss:  1.7509330689907074\n","Epoch time -----  0.6308486461639404  sec\n","Epoch:  5\n","Train loss:  1.636934268474579\n","Validation loss:  1.745132604241371\n","Epoch time -----  0.6167666912078857  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6301623106002807\n","Validation loss:  1.7421402871608733\n","Epoch time -----  0.6224942207336426  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6188998579978944\n","Validation loss:  1.7420506238937379\n","Epoch time -----  0.6291780471801758  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.6126852989196778\n","Validation loss:  1.7405009269714355\n","Epoch time -----  0.618783712387085  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.605072557926178\n","Validation loss:  1.7357912868261338\n","Epoch time -----  0.6272037029266357  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.597485899925232\n","Validation loss:  1.7346277415752411\n","Epoch time -----  0.6102359294891357  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.587705171108246\n","Validation loss:  1.7320845961570739\n","Epoch time -----  0.6156637668609619  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  76.97\n","Round:  7\n","Using entropy sampling on  39394  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  134  points\n","Using loss sampling on  107  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  692\n","Training samples:  692\n","Training\n","Epoch:  1\n","Train loss:  1.6714301000941882\n","Validation loss:  1.7310047566890716\n","Epoch time -----  0.8592844009399414  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.654223452914845\n","Validation loss:  1.7299891144037247\n","Epoch time -----  0.8475842475891113  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6432540308345447\n","Validation loss:  1.7244526416063308\n","Epoch time -----  0.7562568187713623  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6363594857129184\n","Validation loss:  1.7211759239435196\n","Epoch time -----  0.648202657699585  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6248301375995984\n","Validation loss:  1.7218874663114547\n","Epoch time -----  0.616461992263794  sec\n","Epoch:  6\n","Train loss:  1.6178925362500278\n","Validation loss:  1.7150058507919312\n","Epoch time -----  0.6296348571777344  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6113052043047817\n","Validation loss:  1.713431614637375\n","Epoch time -----  0.614079475402832  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.605249133977023\n","Validation loss:  1.7129669427871703\n","Epoch time -----  0.6299052238464355  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.6002423329786821\n","Validation loss:  1.7118452191352844\n","Epoch time -----  0.6042981147766113  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5942489450628108\n","Validation loss:  1.7088786751031875\n","Epoch time -----  0.6113986968994141  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5895643667741255\n","Validation loss:  1.7077889680862426\n","Epoch time -----  0.6305451393127441  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  78.64\n","Round:  8\n","Using entropy sampling on  39308  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  135  points\n","Using loss sampling on  110  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  781\n","Training samples:  781\n","Training\n","Epoch:  1\n","Train loss:  1.6622311518742487\n","Validation loss:  1.7035203903913498\n","Epoch time -----  0.8864834308624268  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6465383492983305\n","Validation loss:  1.7019922226667403\n","Epoch time -----  0.8635556697845459  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6370175434992864\n","Validation loss:  1.700731322169304\n","Epoch time -----  0.8002066612243652  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6233111803348248\n","Validation loss:  1.6969481378793716\n","Epoch time -----  0.6460437774658203  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6190168582476103\n","Validation loss:  1.6907445937395096\n","Epoch time -----  0.6316723823547363  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6122091091596162\n","Validation loss:  1.6899282574653625\n","Epoch time -----  0.6553430557250977  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.6003320033733661\n","Validation loss:  1.688883873820305\n","Epoch time -----  0.6426661014556885  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.593715997842642\n","Validation loss:  1.6863677948713303\n","Epoch time -----  0.6367683410644531  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5930265279916616\n","Validation loss:  1.6857586175203323\n","Epoch time -----  0.6496334075927734  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5922931707822359\n","Validation loss:  1.682932159304619\n","Epoch time -----  0.6400225162506104  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.589257524563716\n","Validation loss:  1.6819749832153321\n","Epoch time -----  0.6316065788269043  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.25\n","Round:  9\n","Using entropy sampling on  39219  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  136  points\n","Using loss sampling on  108  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  868\n","Training samples:  868\n","Training\n","Epoch:  1\n","Train loss:  1.65222487279347\n","Validation loss:  1.6802657753229142\n","Epoch time -----  0.8588817119598389  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.636721100125994\n","Validation loss:  1.6775647103786469\n","Epoch time -----  0.8798058032989502  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.62452266897474\n","Validation loss:  1.67752366065979\n","Epoch time -----  0.8689277172088623  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.616305683340345\n","Validation loss:  1.671995621919632\n","Epoch time -----  0.6678881645202637  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.6097343478884016\n","Validation loss:  1.6708793431520461\n","Epoch time -----  0.6238360404968262  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6039070316723414\n","Validation loss:  1.6682380527257918\n","Epoch time -----  0.6267192363739014  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.5993201051439558\n","Validation loss:  1.668167582154274\n","Epoch time -----  0.6155405044555664  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.593923704964774\n","Validation loss:  1.6663205116987228\n","Epoch time -----  0.6488206386566162  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5917547600609916\n","Validation loss:  1.6658988416194915\n","Epoch time -----  0.6090469360351562  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5872991170201982\n","Validation loss:  1.6645689904689789\n","Epoch time -----  0.6107139587402344  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5834737420082092\n","Validation loss:  1.6624917834997177\n","Epoch time -----  0.6478021144866943  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  79.99\n","Round:  10\n","Using entropy sampling on  39132  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  138  points\n","Using loss sampling on  112  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_54172/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  959\n","Training samples:  959\n","Training\n","Epoch:  1\n","Train loss:  1.642083994547526\n","Validation loss:  1.6601160794496537\n","Epoch time -----  0.8504838943481445  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.6274529059727987\n","Validation loss:  1.6587822318077088\n","Epoch time -----  0.8995959758758545  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.6177971839904786\n","Validation loss:  1.6555938303470612\n","Epoch time -----  0.8745460510253906  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.6105762004852295\n","Validation loss:  1.6547764927148818\n","Epoch time -----  0.8149230480194092  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.605657156308492\n","Validation loss:  1.6516445755958558\n","Epoch time -----  0.6955840587615967  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.6012707471847534\n","Validation loss:  1.651684957742691\n","Epoch time -----  0.6150002479553223  sec\n","Epoch:  7\n","Train loss:  1.597223416964213\n","Validation loss:  1.6505787938833236\n","Epoch time -----  0.6498370170593262  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.5894859472910563\n","Validation loss:  1.649317091703415\n","Epoch time -----  0.6663467884063721  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.5860574165980021\n","Validation loss:  1.6488402873277663\n","Epoch time -----  0.6290614604949951  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.5814603885014853\n","Validation loss:  1.6471920967102052\n","Epoch time -----  0.622194766998291  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.5775780200958252\n","Validation loss:  1.6461805135011673\n","Epoch time -----  0.6317367553710938  sec\n","validation loss minimum, saving model\n","Epoch:  12\n","Train loss:  1.5755321900049846\n","Validation loss:  1.6457632929086685\n","Epoch time -----  0.658219575881958  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10/10 [30:04<00:00, 180.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  80.77\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# kl_marich = []\n","# dist_marich = []\n","# agree_marich = []\n","acc_marich = []\n","\n","for i in tqdm(range(10)):\n","  log_attack = LogisticRegression(784,10)\n","  tl_log, vl_log, tal_log, samp_log = marich(log_attack, unlab_dataset_train, validloader, testloader, budget = 80, init_points = 100, rounds = 10, epochs = 10, LR = 0.02, model_name = \"cifar_log\"+str(i)+\".pt\", sampling = \"all_egl\", device = \"cuda\", model_type = \"log_reg\")\n","  \n","  acc_marich.append(tal_log)\n","  # kl_marich.append(kl_log)\n","  # dist_marich.append(dist)\n","  # agree_marich.append(agree)\n","  np.save(\"./results/acc_marich_lr_cifar.npy\", np.array(acc_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/acc_marich.npy\", np.array(acc_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/kl_marich.npy\", np.array(kl_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/dist_marich.npy\", np.array(dist_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/agree_marich.npy\", np.array(agree_marich))\n","np.save(\"./results/samp_lr_cifar.npy\", np.array(samp_log))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
