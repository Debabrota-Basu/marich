{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1334,"status":"ok","timestamp":1683836960024,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"FCNYmftHHMRy","outputId":"009c31bd-00ad-4d2c-91df-010c34943cc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]}],"source":["from utils import marich, dataset\n","from nets import *\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import pickle\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7542,"status":"ok","timestamp":1683836967564,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"VWTlQi4DHMRy","outputId":"5b22dd97-afbd-43ed-f56a-a4890e234253"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["logreg = LogisticRegression(784,10).cuda()\n","logreg.load_state_dict(torch.load(\"./targets/logreg_mnist.pt\"))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# emnist = datasets.EMNIST('./emnist/train/', download=True, train=True, split = \"letters\", transform=transform)\n","# emnist,_ = random_split(emnist, [50000,74800])"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683836221506,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"mCQwTA1RHMRz"},"outputs":[],"source":["# unlab_x = []\n","# unlab_y = []\n","# for j,k in emnist:\n","#     unlab_x.append(j)\n","#     unlab_y.append(torch.argmax(logreg(j.to(device).view(j.size(0),-1))).cpu())"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# unlab_x = torch.stack(unlab_x)\n","# unlab_y = torch.tensor(unlab_y)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1683836967565,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"4dJ2tDfLe9bd"},"outputs":[],"source":["# with open('./imagenet_target/emnist_lr_x.pkl', 'wb') as f:\n","#   pickle.dump(unlab_x, f)\n","\n","# with open('./imagenet_target/emnist_lr_y.pkl', 'wb') as f:\n","#   pickle.dump(unlab_y, f)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yt6PfwWae9be"},"outputs":[],"source":["with open('./imagenet_target/emnist_lr_x.pkl', 'rb') as f:\n","  unlab_x = pickle.load(f)\n","\n","with open('./imagenet_target/emnist_lr_y.pkl', 'rb') as f:\n","  unlab_y = pickle.load(f)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class basic_dataset(Dataset):\n","    def __init__(self, X, Y = None, transform = None):\n","        self.X = X\n","        self.Y = Y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        if self.Y != None:\n","            y = self.Y[idx]\n","            return x, y\n","        else:\n","            return x\n","\n","class dataset2(Dataset):\n","    def __init__(self, X, Y = None, transform = None):\n","        self.X = X\n","        self.Y = Y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        if self.Y != None:\n","            y = self.Y[idx]\n","            return x, y\n","        else:\n","            return x\n","    def get_data(self,idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        return x\n","    def get_dataset(self, idx):\n","        x = self.X[idx]\n","        y = torch.tensor(self.Y)[idx]\n","        return basic_dataset(x,y)\n","    def get_label(self,idx):\n","        y = torch.tensor(self.Y)[idx]\n","        return y\n","    def get_data_label_loader(self,idx):\n","        if self.transform:\n","            x = self.transform(self.X[idx])\n","        else:\n","            x = self.X[idx]\n","        y = torch.tensor(self.Y)[idx]\n","        \n","        return_set = basic_dataset(x,y)\n","        return torch.utils.data.DataLoader(return_set, batch_size = 64)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["train_id, val_id = train_test_split(range(50000), test_size = 0.2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["unlab_dataset_train = dataset2(unlab_x[train_id], unlab_y[train_id])\n","unlab_dataset_val = dataset2(unlab_x[val_id], unlab_y[val_id])"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"G42zMl0wHMR0"},"outputs":[],"source":["testset = datasets.MNIST('../mnist/train/', download=True, train=False, transform=transform)\n","# trainset, testset = torch.utils.data.random_split(trainset, [50000, 10000])\n","# valset = datasets.MNIST('../mnist/test/', download=True, train=False, transform=transform)\n","testloader = DataLoader(testset, batch_size = 256)\n","validloader = DataLoader(unlab_dataset_val, batch_size = 256)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":30291,"status":"error","timestamp":1683754979048,"user":{"displayName":"Pratik Karmakar","userId":"12296951678310684387"},"user_tz":-480},"id":"klbgtKOBHMR1","outputId":"7fd122fb-dba0-4755-ded3-5588bb007a55"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n"]},{"name":"stdout","output_type":"stream","text":["Using CUDA\n","Test accuracy:  8.32\n","Training samples:  300\n","Epoch:  1\n","Train loss:  2.2764904499053955\n","Validation loss:  2.2073332548141478\n","Epoch time -----  0.6639907360076904  sec\n","Epoch:  2\n","Train loss:  2.1754884243011476\n","Validation loss:  2.1002582907676697\n","Epoch time -----  0.6663820743560791  sec\n","Epoch:  3\n","Train loss:  2.0784732341766357\n","Validation loss:  2.041681948304176\n","Epoch time -----  0.6662769317626953  sec\n","Epoch:  4\n","Train loss:  2.0164656162261965\n","Validation loss:  2.0020285189151763\n","Epoch time -----  0.6660115718841553  sec\n","Epoch:  5\n","Train loss:  1.9630276918411256\n","Validation loss:  1.9611162215471267\n","Epoch time -----  0.6672046184539795  sec\n","Epoch:  6\n","Train loss:  1.9272534608840943\n","Validation loss:  1.9403826594352722\n","Epoch time -----  0.650808572769165  sec\n","Epoch:  7\n","Train loss:  1.9059816598892212\n","Validation loss:  1.919881471991539\n","Epoch time -----  0.6653671264648438  sec\n","Epoch:  8\n","Train loss:  1.8827941179275514\n","Validation loss:  1.9101383358240127\n","Epoch time -----  0.6500680446624756  sec\n","Epoch:  9\n","Train loss:  1.862369990348816\n","Validation loss:  1.8999876022338866\n","Epoch time -----  0.6508610248565674  sec\n","Epoch:  10\n","Train loss:  1.848991298675537\n","Validation loss:  1.8919169664382935\n","Epoch time -----  0.6664557456970215  sec\n","Test accuracy:  43.79\n","Round:  1\n","Using entropy sampling on  39700  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  394  points\n","Using loss sampling on  316  points\n","Budget:  554\n","Training samples:  554\n","Training\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch:  1\n","Train loss:  2.051439536942376\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.8964191794395446\n","Epoch time -----  0.7177848815917969  sec\n","Epoch:  2\n","Train loss:  2.03850355413225\n","Validation loss:  1.8808943152427673\n","Epoch time -----  0.7002780437469482  sec\n","Epoch:  3\n","Train loss:  1.9773917463090684\n","Validation loss:  1.8492908358573914\n","Epoch time -----  0.7432839870452881  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8761013878716364\n","Validation loss:  1.9039825588464736\n","Epoch time -----  0.7205212116241455  sec\n","Epoch:  5\n","Train loss:  1.8584546910391913\n","Validation loss:  1.8426747649908066\n","Epoch time -----  0.6630399227142334  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.8232270346747503\n","Validation loss:  1.814093068242073\n","Epoch time -----  0.6681568622589111  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7962192429436579\n","Validation loss:  1.8128384619951248\n","Epoch time -----  0.6793158054351807  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7784106334050496\n","Validation loss:  1.8024918586015701\n","Epoch time -----  0.6643846035003662  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7647878064049616\n","Validation loss:  1.795301678776741\n","Epoch time -----  0.665480375289917  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.742723782857259\n","Validation loss:  1.790630504488945\n","Epoch time -----  0.6636416912078857  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  58.89\n","Round:  2\n","Using entropy sampling on  39446  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  398  points\n","Using loss sampling on  319  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  810\n","Training samples:  810\n","Training\n","Epoch:  1\n","Train loss:  1.8855969905853271\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.7813172847032548\n","Epoch time -----  0.6912786960601807  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8599811792373657\n","Validation loss:  1.7641267716884612\n","Epoch time -----  0.6399788856506348  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8418052746699407\n","Validation loss:  1.7583971351385117\n","Epoch time -----  0.6406092643737793  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8176020658933199\n","Validation loss:  1.7475361675024033\n","Epoch time -----  0.6149978637695312  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7999284725922804\n","Validation loss:  1.7441410005092621\n","Epoch time -----  0.6409573554992676  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7877904726908758\n","Validation loss:  1.7373425990343094\n","Epoch time -----  0.6628987789154053  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7748847191150372\n","Validation loss:  1.7335152745246887\n","Epoch time -----  0.6738400459289551  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7625447511672974\n","Validation loss:  1.7315897732973098\n","Epoch time -----  0.6651287078857422  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7510355344185462\n","Validation loss:  1.7231061726808548\n","Epoch time -----  0.6827352046966553  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7504658607336192\n","Validation loss:  1.7253442794084548\n","Epoch time -----  0.6640110015869141  sec\n","Testing\n","Test accuracy:  69.19\n","Round:  3\n","Using entropy sampling on  39190  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  402  points\n","Using loss sampling on  322  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1068\n","Training samples:  1068\n","Training\n","Epoch:  1\n","Train loss:  1.8572250394260181\n","Validation loss:  1.724309691786766\n","Epoch time -----  0.727301836013794  sec\n","Epoch:  2\n","Train loss:  1.8336189354167265\n","Validation loss:  1.7147924154996872\n","Epoch time -----  0.7153003215789795  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8170415373409496\n","Validation loss:  1.7070775479078293\n","Epoch time -----  0.6811330318450928  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.807289768667782\n","Validation loss:  1.7059812277555466\n","Epoch time -----  0.6826863288879395  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.79638452389661\n","Validation loss:  1.6968520134687424\n","Epoch time -----  0.7164719104766846  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7869394176146562\n","Validation loss:  1.696910735964775\n","Epoch time -----  0.6979596614837646  sec\n","Epoch:  7\n","Train loss:  1.779791502391591\n","Validation loss:  1.692751607298851\n","Epoch time -----  0.67405104637146  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7723808709312887\n","Validation loss:  1.6873570203781127\n","Epoch time -----  0.6733126640319824  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7646559687221752\n","Validation loss:  1.6881669878959655\n","Epoch time -----  0.681800127029419  sec\n","Epoch:  10\n","Train loss:  1.75649472545175\n","Validation loss:  1.6807452201843263\n","Epoch time -----  0.6830906867980957  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  69.76\n","Round:  4\n","Using entropy sampling on  38932  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  406  points\n","Using loss sampling on  325  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1328\n","Training samples:  1328\n","Training\n","Epoch:  1\n","Train loss:  1.8379527103333246\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6864422380924224\n","Epoch time -----  0.7086977958679199  sec\n","Epoch:  2\n","Train loss:  1.8201734849384852\n","Validation loss:  1.680536925792694\n","Epoch time -----  0.6861743927001953  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8088413760775612\n","Validation loss:  1.6697753727436067\n","Epoch time -----  0.6764225959777832  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8001912264596849\n","Validation loss:  1.6837528437376021\n","Epoch time -----  0.6826426982879639  sec\n","Epoch:  5\n","Train loss:  1.7893766448611306\n","Validation loss:  1.6651447117328644\n","Epoch time -----  0.7335748672485352  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7867577303023565\n","Validation loss:  1.6637241333723067\n","Epoch time -----  0.6973092555999756  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7763200657708305\n","Validation loss:  1.6633538782596589\n","Epoch time -----  0.6987888813018799  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.768880775996617\n","Validation loss:  1.6636052519083022\n","Epoch time -----  0.7140853404998779  sec\n","Epoch:  9\n","Train loss:  1.7663513194947016\n","Validation loss:  1.6604066252708436\n","Epoch time -----  0.700610876083374  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7597631613413494\n","Validation loss:  1.6558726668357848\n","Epoch time -----  0.7027251720428467  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  70.96\n","Round:  5\n","Using entropy sampling on  38672  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  410  points\n","Using loss sampling on  334  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1596\n","Training samples:  1596\n","Training\n","Epoch:  1\n","Train loss:  1.8229443216323853\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6557362765073775\n","Epoch time -----  0.7283177375793457  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8090486240386963\n","Validation loss:  1.6521192163228988\n","Epoch time -----  0.7308659553527832  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.800020055770874\n","Validation loss:  1.6488136857748033\n","Epoch time -----  0.7131297588348389  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7930690574645995\n","Validation loss:  1.6467092692852021\n","Epoch time -----  0.7150869369506836  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7918592262268067\n","Validation loss:  1.638083466887474\n","Epoch time -----  0.6982150077819824  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7869915246963501\n","Validation loss:  1.6402138352394104\n","Epoch time -----  0.6977646350860596  sec\n","Epoch:  7\n","Train loss:  1.7768717575073243\n","Validation loss:  1.639840868115425\n","Epoch time -----  0.7158060073852539  sec\n","Epoch:  8\n","Train loss:  1.7710081005096436\n","Validation loss:  1.6366216033697127\n","Epoch time -----  0.7002253532409668  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.764666771888733\n","Validation loss:  1.6337020844221115\n","Epoch time -----  0.6985266208648682  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7602422666549682\n","Validation loss:  1.6352180123329163\n","Epoch time -----  0.7343301773071289  sec\n","Epoch:  11\n","Train loss:  1.7595261669158935\n","Validation loss:  1.636494341492653\n","Epoch time -----  0.696624755859375  sec\n","Testing\n","Test accuracy:  71.55\n","Round:  6\n","Using entropy sampling on  38404  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  414  points\n","Using loss sampling on  331  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1861\n","Training samples:  1861\n","Training\n","Epoch:  1\n","Train loss:  1.8083329598108928\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.632371574640274\n","Epoch time -----  0.6896610260009766  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7913647452990213\n","Validation loss:  1.6257998198270798\n","Epoch time -----  0.6491425037384033  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8020944317181906\n","Validation loss:  1.6246882230043411\n","Epoch time -----  0.6634781360626221  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.790265425046285\n","Validation loss:  1.6250577956438064\n","Epoch time -----  0.6674273014068604  sec\n","Epoch:  5\n","Train loss:  1.7756563266118368\n","Validation loss:  1.621699246764183\n","Epoch time -----  0.6802282333374023  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.77805362145106\n","Validation loss:  1.6251135349273682\n","Epoch time -----  0.6719882488250732  sec\n","Epoch:  7\n","Train loss:  1.7768336216608682\n","Validation loss:  1.6216243386268616\n","Epoch time -----  0.6771624088287354  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.780468495686849\n","Validation loss:  1.6213110625743865\n","Epoch time -----  0.6992251873016357  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7620531876881917\n","Validation loss:  1.6192543864250184\n","Epoch time -----  0.7098207473754883  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.764394458134969\n","Validation loss:  1.6199090242385865\n","Epoch time -----  0.7199001312255859  sec\n","Epoch:  11\n","Train loss:  1.7596015691757203\n","Validation loss:  1.6143653482198714\n","Epoch time -----  0.7045350074768066  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.74\n","Round:  7\n","Using entropy sampling on  38139  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  418  points\n","Using loss sampling on  336  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2130\n","Training samples:  2130\n","Training\n","Epoch:  1\n","Train loss:  1.803840062197517\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6145853042602538\n","Epoch time -----  0.7569918632507324  sec\n","Epoch:  2\n","Train loss:  1.7922525125391342\n","Validation loss:  1.6157747864723206\n","Epoch time -----  0.7168416976928711  sec\n","Epoch:  3\n","Train loss:  1.7875277119524338\n","Validation loss:  1.6128927558660506\n","Epoch time -----  0.6407229900360107  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7811109774252947\n","Validation loss:  1.6142777532339097\n","Epoch time -----  0.6465902328491211  sec\n","Epoch:  5\n","Train loss:  1.7767097213689018\n","Validation loss:  1.6066719263792038\n","Epoch time -----  0.6615116596221924  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7754877700525171\n","Validation loss:  1.606839570403099\n","Epoch time -----  0.6775100231170654  sec\n","Epoch:  7\n","Train loss:  1.7723094505422257\n","Validation loss:  1.6084625124931335\n","Epoch time -----  0.6741611957550049  sec\n","Epoch:  8\n","Train loss:  1.7681988372522242\n","Validation loss:  1.604347860813141\n","Epoch time -----  0.6650800704956055  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.763024610631606\n","Validation loss:  1.6023291647434235\n","Epoch time -----  0.705665111541748  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7603324616656584\n","Validation loss:  1.6046612203121184\n","Epoch time -----  0.736191987991333  sec\n","Epoch:  11\n","Train loss:  1.7591925123158623\n","Validation loss:  1.6084412574768066\n","Epoch time -----  0.7149631977081299  sec\n","Testing\n","Test accuracy:  71.99\n","Round:  8\n","Using entropy sampling on  37870  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  422  points\n","Using loss sampling on  342  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2404\n","Training samples:  2404\n","Training\n","Epoch:  1\n","Train loss:  1.7901741172138013\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6011009573936463\n","Epoch time -----  0.6807200908660889  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7827826738357544\n","Validation loss:  1.6000269532203675\n","Epoch time -----  0.6766791343688965  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7773193936598928\n","Validation loss:  1.6022197306156158\n","Epoch time -----  0.709883451461792  sec\n","Epoch:  4\n","Train loss:  1.7732338152433698\n","Validation loss:  1.5974159449338914\n","Epoch time -----  0.6654090881347656  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7683948341168856\n","Validation loss:  1.5958662897348403\n","Epoch time -----  0.7364156246185303  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7643961467240985\n","Validation loss:  1.5967704862356187\n","Epoch time -----  0.6832289695739746  sec\n","Epoch:  7\n","Train loss:  1.7615261140622591\n","Validation loss:  1.5935606688261033\n","Epoch time -----  0.766869306564331  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7578585838016711\n","Validation loss:  1.5955296605825424\n","Epoch time -----  0.7514941692352295  sec\n","Epoch:  9\n","Train loss:  1.7565389997080754\n","Validation loss:  1.6003857225179672\n","Epoch time -----  0.7070651054382324  sec\n","Epoch:  10\n","Train loss:  1.7527948398339122\n","Validation loss:  1.5921632677316666\n","Epoch time -----  0.6446852684020996  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.7497139416242902\n","Validation loss:  1.5908710956573486\n","Epoch time -----  0.6483380794525146  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.87\n","Round:  9\n","Using entropy sampling on  37596  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  427  points\n","Using loss sampling on  341  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2677\n","Training samples:  2677\n","Training\n","Epoch:  1\n","Train loss:  1.7817114052318392\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.5943638205528259\n","Epoch time -----  0.6616575717926025  sec\n","Epoch:  2\n","Train loss:  1.7740205072221302\n","Validation loss:  1.589135855436325\n","Epoch time -----  0.6603240966796875  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.766364336013794\n","Validation loss:  1.5901968955993653\n","Epoch time -----  0.6615517139434814  sec\n","Epoch:  4\n","Train loss:  1.7643543652125768\n","Validation loss:  1.5934923768043519\n","Epoch time -----  0.6863842010498047  sec\n","Epoch:  5\n","Train loss:  1.7591061052821932\n","Validation loss:  1.5864470541477202\n","Epoch time -----  0.6539778709411621  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7613525305475508\n","Validation loss:  1.5939020931720733\n","Epoch time -----  0.6575541496276855  sec\n","Epoch:  7\n","Train loss:  1.755755515325637\n","Validation loss:  1.5868445068597794\n","Epoch time -----  0.6766884326934814  sec\n","Epoch:  8\n","Train loss:  1.752420493534633\n","Validation loss:  1.5837588787078858\n","Epoch time -----  0.659203290939331  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7518902279081798\n","Validation loss:  1.586577233672142\n","Epoch time -----  0.6613271236419678  sec\n","Epoch:  10\n","Train loss:  1.7485447639510745\n","Validation loss:  1.5825005501508713\n","Epoch time -----  0.6809420585632324  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.7490431240626745\n","Validation loss:  1.5871794283390046\n","Epoch time -----  0.6572048664093018  sec\n","Testing\n","Test accuracy:  72.18\n","Round:  10\n","Using entropy sampling on  37323  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  431  points\n","Using loss sampling on  344  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2954\n","Training samples:  2954\n","Training\n","Epoch:  1\n","Train loss:  1.7705508876354137\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.5797342032194137\n","Epoch time -----  0.6745460033416748  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7601417759631544\n","Validation loss:  1.5801854729652405\n","Epoch time -----  0.6611590385437012  sec\n","Epoch:  3\n","Train loss:  1.7596310098120507\n","Validation loss:  1.5824259966611862\n","Epoch time -----  0.6413979530334473  sec\n","Epoch:  4\n","Train loss:  1.7569077902651848\n","Validation loss:  1.5787660241127015\n","Epoch time -----  0.6655917167663574  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7490737412838226\n","Validation loss:  1.578587254881859\n","Epoch time -----  0.6820197105407715  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7495870514118925\n","Validation loss:  1.58152132332325\n","Epoch time -----  0.6914079189300537  sec\n","Epoch:  7\n","Train loss:  1.7479571788869006\n","Validation loss:  1.5756576895713805\n","Epoch time -----  0.6583156585693359  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7445273272534634\n","Validation loss:  1.5759342938661576\n","Epoch time -----  0.661862850189209  sec\n","Epoch:  9\n","Train loss:  1.746661016281615\n","Validation loss:  1.577695646882057\n","Epoch time -----  0.659726619720459  sec\n","Epoch:  10\n","Train loss:  1.7407247183170724\n","Validation loss:  1.5751026809215545\n","Epoch time -----  0.6492938995361328  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.7357160106618355\n","Validation loss:  1.5782588362693786\n","Epoch time -----  0.6822104454040527  sec\n","Epoch:  12\n","Train loss:  1.736327295607709\n","Validation loss:  1.572731676697731\n","Epoch time -----  0.7380867004394531  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1/2 [02:59<02:59, 179.72s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  72.08\n","Using CUDA\n","Test accuracy:  9.38\n","Training samples:  300\n","Epoch:  1\n","Train loss:  2.271845245361328\n","Validation loss:  2.2123924672603605\n","Epoch time -----  0.6057553291320801  sec\n","Epoch:  2\n","Train loss:  2.1733920574188232\n","Validation loss:  2.1253249526023863\n","Epoch time -----  0.629408597946167  sec\n","Epoch:  3\n","Train loss:  2.0942270278930666\n","Validation loss:  2.063840442895889\n","Epoch time -----  0.5776998996734619  sec\n","Epoch:  4\n","Train loss:  2.0470101833343506\n","Validation loss:  2.0303532093763352\n","Epoch time -----  0.574831485748291  sec\n","Epoch:  5\n","Train loss:  1.9982937574386597\n","Validation loss:  2.0098464131355285\n","Epoch time -----  0.5705838203430176  sec\n","Epoch:  6\n","Train loss:  1.9784881353378296\n","Validation loss:  1.9786974430084228\n","Epoch time -----  0.5741972923278809  sec\n","Epoch:  7\n","Train loss:  1.946238899230957\n","Validation loss:  1.9618499010801316\n","Epoch time -----  0.5851922035217285  sec\n","Epoch:  8\n","Train loss:  1.9173283100128173\n","Validation loss:  1.9439819544553756\n","Epoch time -----  0.6723825931549072  sec\n","Epoch:  9\n","Train loss:  1.8812028646469117\n","Validation loss:  1.9522162288427354\n","Epoch time -----  0.6467998027801514  sec\n","Epoch:  10\n","Train loss:  1.8864149093627929\n","Validation loss:  1.930891326069832\n","Epoch time -----  0.5998680591583252  sec\n","Test accuracy:  40.31\n","Round:  1\n","Using entropy sampling on  39700  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  394  points\n","Using loss sampling on  315  points\n","Budget:  552\n","Training samples:  552\n","Training\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch:  1\n","Train loss:  2.027774201499091\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.9130290120840072\n","Epoch time -----  0.6566846370697021  sec\n","Epoch:  2\n","Train loss:  1.9978317817052205\n","Validation loss:  1.8976120710372926\n","Epoch time -----  0.6012156009674072  sec\n","Epoch:  3\n","Train loss:  1.9320385720994737\n","Validation loss:  1.8628048330545426\n","Epoch time -----  0.6109685897827148  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8839384847217135\n","Validation loss:  1.8733709305524826\n","Epoch time -----  0.6094505786895752  sec\n","Epoch:  5\n","Train loss:  1.8736722336875067\n","Validation loss:  1.8516233563423157\n","Epoch time -----  0.6115334033966064  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.844971285925971\n","Validation loss:  1.8413629084825516\n","Epoch time -----  0.6040492057800293  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.8378978702757094\n","Validation loss:  1.8333458393812179\n","Epoch time -----  0.6113953590393066  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.8137922816806369\n","Validation loss:  1.8299146384000777\n","Epoch time -----  0.637627124786377  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.8056900368796454\n","Validation loss:  1.8223600476980208\n","Epoch time -----  0.6168258190155029  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7868284781773884\n","Validation loss:  1.8078379034996033\n","Epoch time -----  0.6157453060150146  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  54.73\n","Round:  2\n","Using entropy sampling on  39448  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  398  points\n","Using loss sampling on  320  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  809\n","Training samples:  809\n","Training\n","Epoch:  1\n","Train loss:  1.9058836148335383\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.8016869187355042\n","Epoch time -----  0.6934716701507568  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8623078694710364\n","Validation loss:  1.791680908203125\n","Epoch time -----  0.6061503887176514  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8297143991176898\n","Validation loss:  1.7781845480203629\n","Epoch time -----  0.6064848899841309  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.8134269255858202\n","Validation loss:  1.7663495749235154\n","Epoch time -----  0.6236729621887207  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.792136549949646\n","Validation loss:  1.7574955374002457\n","Epoch time -----  0.6204056739807129  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.777280303148123\n","Validation loss:  1.7515828102827071\n","Epoch time -----  0.6631889343261719  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7644856251203096\n","Validation loss:  1.7418177336454392\n","Epoch time -----  0.6263384819030762  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7527053539569561\n","Validation loss:  1.74051473736763\n","Epoch time -----  0.6655864715576172  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7455367675194373\n","Validation loss:  1.734292632341385\n","Epoch time -----  0.6241636276245117  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7374071341294508\n","Validation loss:  1.7331732600927352\n","Epoch time -----  0.6408650875091553  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  68.17\n","Round:  3\n","Using entropy sampling on  39191  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  402  points\n","Using loss sampling on  322  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1067\n","Training samples:  1067\n","Training\n","Epoch:  1\n","Train loss:  1.8440818295759314\n","Validation loss:  1.7232732385396958\n","Epoch time -----  0.6767892837524414  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8260818579617668\n","Validation loss:  1.7187001168727876\n","Epoch time -----  0.6478626728057861  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8096022676019108\n","Validation loss:  1.7146911084651948\n","Epoch time -----  0.6356761455535889  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7995614795123829\n","Validation loss:  1.7080969333648681\n","Epoch time -----  0.6625180244445801  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7833943016388838\n","Validation loss:  1.706296655535698\n","Epoch time -----  0.6456379890441895  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7754547034992891\n","Validation loss:  1.7013633131980896\n","Epoch time -----  0.6941201686859131  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.768603619407205\n","Validation loss:  1.693753433227539\n","Epoch time -----  0.6347496509552002  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7623407209620756\n","Validation loss:  1.6974451124668122\n","Epoch time -----  0.6994683742523193  sec\n","Epoch:  9\n","Train loss:  1.7540578000685747\n","Validation loss:  1.6930524408817291\n","Epoch time -----  0.698876142501831  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7448753118515015\n","Validation loss:  1.6909172594547273\n","Epoch time -----  0.6653966903686523  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  69.65\n","Round:  4\n","Using entropy sampling on  38933  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  406  points\n","Using loss sampling on  328  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1330\n","Training samples:  1330\n","Training\n","Epoch:  1\n","Train loss:  1.831336640176319\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6949528247117995\n","Epoch time -----  0.6592659950256348  sec\n","Epoch:  2\n","Train loss:  1.8189124096007574\n","Validation loss:  1.6812361747026443\n","Epoch time -----  0.6857202053070068  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8057637271426974\n","Validation loss:  1.6746973991394043\n","Epoch time -----  0.6525063514709473  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.797029245467413\n","Validation loss:  1.6762473106384277\n","Epoch time -----  0.6785337924957275  sec\n","Epoch:  5\n","Train loss:  1.7863171725046068\n","Validation loss:  1.6688452750444411\n","Epoch time -----  0.6157264709472656  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7786801372255598\n","Validation loss:  1.6677622854709626\n","Epoch time -----  0.6238467693328857  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7745769648324876\n","Validation loss:  1.6627576291561126\n","Epoch time -----  0.6478924751281738  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.764805481547401\n","Validation loss:  1.6659340679645538\n","Epoch time -----  0.6267075538635254  sec\n","Epoch:  9\n","Train loss:  1.7593801532472884\n","Validation loss:  1.660571175813675\n","Epoch time -----  0.6237428188323975  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7555511451902843\n","Validation loss:  1.660520812869072\n","Epoch time -----  0.6172959804534912  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.1\n","Round:  5\n","Using entropy sampling on  38670  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  410  points\n","Using loss sampling on  328  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1593\n","Training samples:  1593\n","Training\n","Epoch:  1\n","Train loss:  1.8221290063858033\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6596791177988053\n","Epoch time -----  0.6535801887512207  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8078867244720458\n","Validation loss:  1.6530356645584106\n","Epoch time -----  0.6902658939361572  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.8022629594802857\n","Validation loss:  1.664438083767891\n","Epoch time -----  0.6530835628509521  sec\n","Epoch:  4\n","Train loss:  1.7963060426712036\n","Validation loss:  1.6455327868461609\n","Epoch time -----  0.6317713260650635  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7866786050796508\n","Validation loss:  1.6442273557186127\n","Epoch time -----  0.6251325607299805  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7816662549972535\n","Validation loss:  1.6464595437049865\n","Epoch time -----  0.6559567451477051  sec\n","Epoch:  7\n","Train loss:  1.7774691486358642\n","Validation loss:  1.6392945289611816\n","Epoch time -----  0.7301027774810791  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7733493900299073\n","Validation loss:  1.6375232011079788\n","Epoch time -----  0.6451854705810547  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7630951356887818\n","Validation loss:  1.6350767254829406\n","Epoch time -----  0.654282808303833  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.759360671043396\n","Validation loss:  1.6370167195796967\n","Epoch time -----  0.6940033435821533  sec\n","Epoch:  11\n","Train loss:  1.7588279056549072\n","Validation loss:  1.6321514993906021\n","Epoch time -----  0.6510050296783447  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  71.49\n","Round:  6\n","Using entropy sampling on  38407  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  414  points\n","Using loss sampling on  337  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  1863\n","Training samples:  1863\n","Training\n","Epoch:  1\n","Train loss:  1.8170411586761475\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6318783104419707\n","Epoch time -----  0.713294267654419  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.8058729330698648\n","Validation loss:  1.6358500242233276\n","Epoch time -----  0.6827526092529297  sec\n","Epoch:  3\n","Train loss:  1.8042904019355774\n","Validation loss:  1.6265483111143113\n","Epoch time -----  0.6882586479187012  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7862965941429139\n","Validation loss:  1.622354319691658\n","Epoch time -----  0.6903817653656006  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.7823941866556803\n","Validation loss:  1.6277932673692703\n","Epoch time -----  0.7199358940124512  sec\n","Epoch:  6\n","Train loss:  1.773677659034729\n","Validation loss:  1.6258985251188278\n","Epoch time -----  0.6876258850097656  sec\n","Epoch:  7\n","Train loss:  1.7794285853703817\n","Validation loss:  1.6181861937046051\n","Epoch time -----  0.6563587188720703  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7815197308858235\n","Validation loss:  1.618408876657486\n","Epoch time -----  0.6373662948608398  sec\n","Epoch:  9\n","Train loss:  1.7686522722244262\n","Validation loss:  1.6196507036685943\n","Epoch time -----  0.6521205902099609  sec\n","Epoch:  10\n","Train loss:  1.7676882187525431\n","Validation loss:  1.6177646219730377\n","Epoch time -----  0.6596341133117676  sec\n","validation loss minimum, saving model\n","Epoch:  11\n","Train loss:  1.7650352756182353\n","Validation loss:  1.618030121922493\n","Epoch time -----  0.6656904220581055  sec\n","Testing\n","Test accuracy:  71.5\n","Round:  7\n","Using entropy sampling on  38137  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  418  points\n","Using loss sampling on  334  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2132\n","Training samples:  2132\n","Training\n","Epoch:  1\n","Train loss:  1.8076562460731058\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.6153526872396469\n","Epoch time -----  0.724454402923584  sec\n","validation loss minimum, saving model\n","Epoch:  2\n","Train loss:  1.7917574959642746\n","Validation loss:  1.6098383367061615\n","Epoch time -----  0.6904630661010742  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7850002856815563\n","Validation loss:  1.6100221633911134\n","Epoch time -----  0.6806070804595947  sec\n","Epoch:  4\n","Train loss:  1.7819758828948526\n","Validation loss:  1.607879674434662\n","Epoch time -----  0.6743001937866211  sec\n","validation loss minimum, saving model\n","Epoch:  5\n","Train loss:  1.777982589076547\n","Validation loss:  1.6111322432756423\n","Epoch time -----  0.7082512378692627  sec\n","Epoch:  6\n","Train loss:  1.7753370824982138\n","Validation loss:  1.6087153524160385\n","Epoch time -----  0.6530005931854248  sec\n","Epoch:  7\n","Train loss:  1.7717609861317802\n","Validation loss:  1.6042160034179687\n","Epoch time -----  0.6636569499969482  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7647695155704723\n","Validation loss:  1.6013522356748582\n","Epoch time -----  0.6682991981506348  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7616963561843424\n","Validation loss:  1.6002345949411392\n","Epoch time -----  0.715703010559082  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7611180403653313\n","Validation loss:  1.6013100504875184\n","Epoch time -----  0.6849441528320312  sec\n","Epoch:  11\n","Train loss:  1.757143581614775\n","Validation loss:  1.6011878848075867\n","Epoch time -----  0.6444084644317627  sec\n","Testing\n","Test accuracy:  71.84\n","Round:  8\n","Using entropy sampling on  37868  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  422  points\n","Using loss sampling on  338  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2403\n","Training samples:  2403\n","Training\n","Epoch:  1\n","Train loss:  1.795590052479192\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.602308538556099\n","Epoch time -----  0.6824181079864502  sec\n","Epoch:  2\n","Train loss:  1.7815126745324386\n","Validation loss:  1.6022758036851883\n","Epoch time -----  0.6570589542388916  sec\n","Epoch:  3\n","Train loss:  1.7739253169611882\n","Validation loss:  1.6022372215986251\n","Epoch time -----  0.6456894874572754  sec\n","Epoch:  4\n","Train loss:  1.773607821840989\n","Validation loss:  1.6023179024457932\n","Epoch time -----  0.6470203399658203  sec\n","Epoch:  5\n","Train loss:  1.7754060657400834\n","Validation loss:  1.5971170842647553\n","Epoch time -----  0.6465427875518799  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.7700000499424182\n","Validation loss:  1.5963527619838715\n","Epoch time -----  0.658043622970581  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7638218277379085\n","Validation loss:  1.5938304990530014\n","Epoch time -----  0.6674325466156006  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7605115018392865\n","Validation loss:  1.5914827764034272\n","Epoch time -----  0.6577897071838379  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.7571626776143123\n","Validation loss:  1.5936267852783204\n","Epoch time -----  0.6443817615509033  sec\n","Epoch:  10\n","Train loss:  1.755664392521507\n","Validation loss:  1.594071054458618\n","Epoch time -----  0.6428666114807129  sec\n","Epoch:  11\n","Train loss:  1.750509660494955\n","Validation loss:  1.5888058185577392\n","Epoch time -----  0.6382362842559814  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.03\n","Round:  9\n","Using entropy sampling on  37597  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  427  points\n","Using loss sampling on  343  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2678\n","Training samples:  2678\n","Training\n","Epoch:  1\n","Train loss:  1.788090447584788\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.5952802449464798\n","Epoch time -----  0.6689395904541016  sec\n","Epoch:  2\n","Train loss:  1.7766106809888567\n","Validation loss:  1.5881561934947968\n","Epoch time -----  0.6971428394317627  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7720767287980943\n","Validation loss:  1.5871092826128006\n","Epoch time -----  0.7218832969665527  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7683780931291126\n","Validation loss:  1.5898469895124436\n","Epoch time -----  0.7355189323425293  sec\n","Epoch:  5\n","Train loss:  1.7663110494613647\n","Validation loss:  1.5834580272436143\n","Epoch time -----  0.7006902694702148  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.762178035009475\n","Validation loss:  1.5827132523059846\n","Epoch time -----  0.6664185523986816  sec\n","validation loss minimum, saving model\n","Epoch:  7\n","Train loss:  1.7602796015285311\n","Validation loss:  1.58568075299263\n","Epoch time -----  0.6502666473388672  sec\n","Epoch:  8\n","Train loss:  1.7600688196363903\n","Validation loss:  1.5832004874944687\n","Epoch time -----  0.6452884674072266  sec\n","Epoch:  9\n","Train loss:  1.7548898458480835\n","Validation loss:  1.5814618647098542\n","Epoch time -----  0.6380758285522461  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7532644442149572\n","Validation loss:  1.5819280296564102\n","Epoch time -----  0.6403982639312744  sec\n","Epoch:  11\n","Train loss:  1.7483102906317938\n","Validation loss:  1.5804472774267198\n","Epoch time -----  0.6925983428955078  sec\n","validation loss minimum, saving model\n","Testing\n","Test accuracy:  72.15\n","Round:  10\n","Using entropy sampling on  37322  points\n"]},{"name":"stderr","output_type":"stream","text":["e:\\pratik\\new_marich\\utils.py:384: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  probs = F.softmax(probs).detach().cpu().numpy()\n"]},{"name":"stdout","output_type":"stream","text":["Using gradient sampling on  431  points\n","Using loss sampling on  344  points\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","e:\\pratik\\new_marich\\nets.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(outputs)\n","e:\\pratik\\new_marich\\utils.py:624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_hat = torch.tensor(y_hat)\n","C:\\Users\\pkpra\\AppData\\Local\\Temp/ipykernel_14760/108530465.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y = torch.tensor(self.Y)[idx]\n","e:\\pratik\\new_marich\\utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels.append(torch.tensor(l))\n"]},{"name":"stdout","output_type":"stream","text":["Budget:  2954\n","Training samples:  2954\n","Training\n","Epoch:  1\n","Train loss:  1.7792484024737745\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Validation loss:  1.5819733530282973\n","Epoch time -----  0.7097575664520264  sec\n","Epoch:  2\n","Train loss:  1.7671609077047794\n","Validation loss:  1.5798051625490188\n","Epoch time -----  0.6688041687011719  sec\n","validation loss minimum, saving model\n","Epoch:  3\n","Train loss:  1.7625338194218088\n","Validation loss:  1.577900266647339\n","Epoch time -----  0.6671490669250488  sec\n","validation loss minimum, saving model\n","Epoch:  4\n","Train loss:  1.7599776907170073\n","Validation loss:  1.5781805336475372\n","Epoch time -----  0.6924688816070557  sec\n","Epoch:  5\n","Train loss:  1.7608909556206236\n","Validation loss:  1.5768379986286163\n","Epoch time -----  0.6682877540588379  sec\n","validation loss minimum, saving model\n","Epoch:  6\n","Train loss:  1.75973634770576\n","Validation loss:  1.5780759662389756\n","Epoch time -----  0.6508595943450928  sec\n","Epoch:  7\n","Train loss:  1.7537849786433768\n","Validation loss:  1.5765389561653138\n","Epoch time -----  0.6440122127532959  sec\n","validation loss minimum, saving model\n","Epoch:  8\n","Train loss:  1.7508933772432043\n","Validation loss:  1.5754379421472549\n","Epoch time -----  0.6396396160125732  sec\n","validation loss minimum, saving model\n","Epoch:  9\n","Train loss:  1.749542512792222\n","Validation loss:  1.5742802441120147\n","Epoch time -----  0.6468219757080078  sec\n","validation loss minimum, saving model\n","Epoch:  10\n","Train loss:  1.7451957134490317\n","Validation loss:  1.575509148836136\n","Epoch time -----  0.643928050994873  sec\n","Epoch:  11\n","Train loss:  1.7465188198901238\n","Validation loss:  1.575994110107422\n","Epoch time -----  0.6645517349243164  sec\n","Epoch:  12\n","Train loss:  1.7479413037604474\n","Validation loss:  1.5727390229701996\n","Epoch time -----  0.6807732582092285  sec\n","validation loss minimum, saving model\n","Testing\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [05:53<00:00, 176.93s/it]"]},{"name":"stdout","output_type":"stream","text":["Test accuracy:  72.06\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# kl_marich = []\n","# dist_marich = []\n","# agree_marich = []\n","# acc_marich = []\n","\n","for i in tqdm(range(8,10)):\n","  log_attack = LogisticRegression(784,10)\n","  tl_log, vl_log, tal_log, samp_log = marich(log_attack, unlab_dataset_train, validloader, testloader, budget = 250, init_points = 300, rounds = 10, epochs = 10, LR = 0.02, model_name = \"emnist_log\"+str(i)+\".pt\", sampling = \"all_egl\", device = \"cuda\")\n","  \n","  acc_marich.append(tal_log)\n","  # kl_marich.append(kl_log)\n","  # dist_marich.append(dist)\n","  # agree_marich.append(agree)\n","  np.save(\"./results/acc_marich_lr_emnist.npy\", np.array(acc_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/acc_marich.npy\", np.array(acc_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/kl_marich.npy\", np.array(kl_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/dist_marich.npy\", np.array(dist_marich))\n","  # np.save(\"/content/drive/MyDrive/Marich/MNIST/kl_dist/agree_marich.npy\", np.array(agree_marich))\n","np.save(\"./results/samp_lr_emnist.npy\", np.array(samp_log))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
